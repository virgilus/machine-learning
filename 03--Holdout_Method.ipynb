{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab4971d8-ab5c-4530-883b-a8f0b723b9fa",
   "metadata": {},
   "source": [
    "# The Holdout Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8549e95-b39e-46d3-8a43-14f2496a8130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(\"data/iowa_housing.csv\")\n",
    "\n",
    "feature_names = [\n",
    "'LotArea', # Total lot area of a property, measured in square feet.\n",
    "'YearBuilt', # Year when the house was constructed or built.\n",
    "'1stFlrSF', # Total square footage of the first (ground) floor of the house.\n",
    "'2ndFlrSF', # Total square footage of the second floor of the house.\n",
    "'BedroomAbvGr', # Number of bedrooms located above the ground level.\n",
    "'TotRmsAbvGrd', # Total number of rooms (excluding bathrooms) above ground level.\n",
    "'GrLivArea', # Above ground living area (square feet)\n",
    "]\n",
    "\n",
    "X = df[feature_names]\n",
    "y = df['SalePrice'] # Same as before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9de1ed8-37bc-4063-a697-8d8a0501e44a",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e977eac-7b37-49fb-bf4a-6ecb632b97cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_model = DecisionTreeRegressor(random_state=42)\n",
    "tree_model.fit(X,y)\n",
    "y_pred = tree_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e3ae7c-e334-43f4-ac40-17ca1976d044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree_model.score(X, y) # R²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56745afd-2d01-4855-ba6a-9f899dba0ada",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(y, y_pred) # Too good to be true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa50b813-be3d-4ac4-b9ee-2c17b9dc49af",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Train / test split\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "The accuracy is excellent, but that's because the way we adjusted our model is incorrect. Since the data we used to train and test the model are the same, it's normal that we get almost only correct answers. We are dealing with a classic case of **overfitting**.\n",
    "\n",
    "### Let's divide the data\n",
    "\n",
    "To evaluate the robustness of a model, we will test it on data it has never seen before. To do this, we will split our data into two groups: one will be used for training (*train*), and the other to test the model (*test*).\n",
    "\n",
    "The \"train_size\" parameter will determine the proportion of our data used for training. A value of 0.8 means that we reserve 80% of our data for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ca367c-9b47-4d0f-8dff-873b0b9953d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Now our data are split in 4 different parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size=0.8)\n",
    "\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_tree = tree_model.predict(X_test)\n",
    "print(mean_absolute_error(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff0dcc9-924a-411d-a6ba-22a77fcbbcb3",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "The MAE is significantly higher, approximately 350 times higher! Since the average price of a house was around $180,000, this means our model is off by about 1/9 of the price. There are, of course, many ways to achieve a higher score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2499d9ce-7b51-418c-9df5-c267aea0c418",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Let's compare with the linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea9c1f8-949a-4525-a6c0-8d315d499c72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "print(mean_absolute_error(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3980bc97-73dd-4e9c-af02-7b15c84493fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "lr_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e7422e-99e6-4822-a878-6bfb13dc7f74",
   "metadata": {
    "tags": []
   },
   "source": [
    "(pd.DataFrame(zip(X.columns, lr_model.coef_), columns=['variable', 'coefficient'])\n",
    "              .sort_values(by='coefficient', ascending=False)\n",
    "              .reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d48172-f1e1-400c-97dd-298da9ecbe27",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Limits of the \"Holdout\" Method: Cross Validation\n",
    "\n",
    "The division between \"train\" and \"test\" has a flaw: it prevents our model from being trained on the entire dataset and is sensitive to the random splitting of the data. Are we not risking to not feed important data to our model?\n",
    "\n",
    "## The *K-Fold Cross Validation*\n",
    "\n",
    "To solve this problem, we can use Cross Validation, along with one of its variants: K-Fold Cross Validation.\n",
    "\n",
    "We will divide the dataset into $K$ equal parts, reserve the first part for testing, and train the model on the rest. Then repeat the same operation, selecting the second part as the test set, and so on.\n",
    "\n",
    "In general, we then calculate the average of all the different scores obtained, which gives us a \"cross-validated\" score, i.e., a hypothetical score that could be achieved if the model were trained on the entire dataset.\n",
    "Note that cross-validation does not produce a trained model; this method only provides a series of scores.\n",
    "\n",
    "## Choosing the Number $K$\n",
    "\n",
    "We often choose a number between 3 and 10. The higher the number, the more representative the final score will be. However, this will require more computational time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7600c7-739a-4fa2-8a28-c8154492a98c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cross validation (K-Fold)\n",
    "<div>\n",
    "<img src=\"files/cross_validation.jpg\" alt=\"cross_validation\" width=\"70%\" align='center' source=\"https://www.50a.fr/img/upload/machine%20learning..jpg\" /> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3334a66-4ba9-45e5-a46a-b2dba061c56b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cross_validate(tree_model, X, y,\n",
    "               #verbose=2,\n",
    "               cv=10) # Default Score is R² for DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58309d00-7ed2-4d45-a60d-63c8666baa53",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ca9fcf-09ba-4aab-a5ab-1b138ff9acbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Our original score with the Tree model based on our initial split\n",
    "tree_model.score(X_test, y_test) # R²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a13e710-cbb0-4ac8-952d-4ed58c146a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Average score with cross validation\n",
    "cross_validate(tree_model, X, y, cv=10)['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd5634-9b99-40ed-aeff-94d0b9b76b93",
   "metadata": {},
   "source": [
    "### LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a5ea57-7d84-4820-85a0-ac92641625de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Our original score with Linear Regression model based on our initial split\n",
    "lr_model.score(X_test, y_test) # score is R²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a61784-e76a-407d-b0a8-4f76f4aa13ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Average score with cross validation\n",
    "cross_validate(lr_model, X, y, cv=10)['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503f8034-f02d-4b17-b90d-707934a027b1",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "# Model parameters\n",
    "\n",
    "A decision tree can be configured in many different ways, as you can see by examining the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) for our type of model.\n",
    "\n",
    "### Difference between parameters and hyperparameters\n",
    "\n",
    "In machine learning, hyperparameters are the parameters that govern the process of generating the internal parameters of the model.\n",
    "\n",
    "For example, in our model, its parameters include all the branches that lead to the leaves of our tree, among other things. These parameters were determined by the model during its training and changed significantly between the start of the fitting process and when it finished.\n",
    "\n",
    "Hyperparameters, on the other hand, are parameters that are often set by a human and are not modified during training. They represent high-level directions or settings.\n",
    "\n",
    "One of the most important hyperparameters for this model is the depth of the tree. For now, we didn't give any specific instructions, so this hyperparameter was generated by the program. Let's examine it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dda2058-2e40-493c-8bdb-ec7887efc074",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model.tree_.max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfe85b0-9446-462d-916a-4b67e12184d0",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "There is a maximum of 25 levels of depth in our tree. Each time we add a level of depth to our tree, we increase its maximum number of leaves and therefore its precision. However the number of houses in each leaf will be reduced, which means that predictions will become less reliable. So, we need to find a balance between precision and reliability.\n",
    "\n",
    "### Overfitting and Underfitting\n",
    "\n",
    "The phenomena of overfitting and underfitting are central concepts in machine learning.\n",
    "\n",
    "- Overfitting occurs when the model's results closely match the data it was trained on but make significant errors when applied to unknown data. This happens when our decision tree is too deep.\n",
    "\n",
    "- Underfitting occurs when the model fails to distinguish essential features in our data. It will have a poor score on both the training data and the test data. This occurs when our model is not deep enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb13e89c-3859-48ba-b448-eb2c3fd32fdd",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"files/underfitting_and_overfitting.png\" width=\"65%\" align='center'/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b56f37-448e-4aeb-84aa-4d4ba3718ae8",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "The graph above shows the variation in MAE based on the depth of a decision tree. The term \"validation\" here refers to the \"test\" dataset. Here are some observations about the graph:\n",
    "\n",
    "- On average, the model will always have a better score when predicting data from its training set rather than unknown data from the test set.\n",
    "\n",
    "- Increasing the depth initially improves the model on both the training and test sets.\n",
    "\n",
    "- There comes a point where increasing the depth improves precision on the training set, but the MAE starts to increase on the test set. This is the phenomenon of overfitting.\n",
    "\n",
    "The goal of hyperparameters is to find this balance point, which should allow us to maximize the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d136da-fcff-4643-9599-9bb08d80d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae(max_depth, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test):\n",
    "    tree_model = DecisionTreeRegressor(max_depth=max_depth, random_state=42)\n",
    "    tree_model.fit(X_train, y_train)\n",
    "    y_pred_train = tree_model.predict(X_train)\n",
    "    y_pred_test = tree_model.predict(X_test)\n",
    "    mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "    mae_validation = mean_absolute_error(y_test, y_pred_test)\n",
    "    \n",
    "    return mae_validation, mae_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee21fc1-8fe2-449c-921c-4bda23e0c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple\n",
    "get_mae(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77b37c4-11d8-4dd6-8447-fd65789132c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for max_depth in range(2, 20):\n",
    "    mae, mae_train = get_mae(max_depth)\n",
    "    d[max_depth] = [mae, mae_train]\n",
    "    print(f\"Max Depth: {max_depth} \\t\\t Mean Absolute Error: {mae, mae_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49433697-600f-4287-bb83-f1c8e59fd2a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(pd.DataFrame.from_dict(d, orient='index')\n",
    "             .rename(columns={0 : 'MAE Validation', 1 : 'MAE Train'})\n",
    "\n",
    ").plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffd16de-268b-4fca-bf58-af7ec3c60b90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Zoom on MAE Validation\n",
    "\n",
    "(pd.DataFrame.from_dict(d, orient='index')\n",
    "             .rename(columns={0 : 'MAE Validation', 1 : 'MAE Train'})['MAE Validation']\n",
    "\n",
    ").plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b40bb2-049f-4c4a-bacd-9dfe9a12b2f6",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## The Bias / Variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd22e358-bb5b-4294-9fcb-ac1002cbd666",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div>\n",
    "<img src=\"files/overfitting_underfitting_good_balance.png\" width=\"80%\" align='center'/ source='https://medium.com/analytics-vidhya/5-ways-to-achieve-right-balance-of-bias-and-variance-in-ml-model-f734fea6116'> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d1397e-ae12-4982-92dd-08149a0b1215",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "- **Variance** is the variability of model predictions for a given input, indicating how much the model's predictions would differ if trained on different subsets of the data.\n",
    "\n",
    "    **High variance** models are more complex and flexible, often capturing noise and fluctuations in the training data. They may perform well on the training data but may not generalize well to new data or the test dataset (**overfitting**).\n",
    "\n",
    "- **Bias** is the error introduced by approximating a complex problem by a much simpler model. It represents the difference between the average prediction of the model and the true value.\n",
    "\n",
    "    **High bias** models tend to be too simplistic and may **underfit** the data. They may not capture the underlying patterns and relationships in the data.\n",
    "\n",
    "    However, they can have relatively good performance when trying to predict values on the test dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff68577-92ed-4562-9b69-77b397b62926",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## No Free Lunch Theorem\n",
    "\n",
    "The term was popularized by David H. Wolpert and William Macready in the late 1990s.\n",
    "\n",
    "The No Free Lunch theorem states that there is no algorithm that performs optimally on all possible problems or datasets.\n",
    "In other words, no algorithm is universally superior, and the performance of an algorithm is highly dependent on the specific characteristics of the problem it is applied to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f0e35d-4de0-4fbd-86c0-5137bb0c6fbb",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div>\n",
    "<img src=\"files/no_free_lunch.png\" width=\"75%\" align='center' source='https://community.alteryx.com/t5/Data-Science/There-is-No-Free-Lunch-in-Data-Science/ba-p/347402'/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f43975-9aa8-4717-b442-a2c7c0ed71bd",
   "metadata": {},
   "source": [
    "## What is the difference between test and validation sets?\n",
    "\n",
    "- The **validation set** is used during the training process to tune hyperparameters and assess the performance of different models. It helps to prevent overfitting by providing an independent dataset that the model has not seen during training.\n",
    "\n",
    "- The **test Set** is used to evaluate the final performance of the selected model, it provides an unbiased estimate of how well the model will generalize to unseen data. The test set should only be used once, after the model has been trained and tuned using the validation set.\n",
    "\n",
    "Depending on what you are doing and the models you're using, you don't necessarily need to create a validation set. We'll get back to that later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a49d876-16ad-4014-bd9a-9024051bf959",
   "metadata": {},
   "source": [
    "## Learning Curves and train/test split size\n",
    "\n",
    "**Do we overfit? Do we underfit? Do we have enough data?**\n",
    "\n",
    "To answer these questions, let's try to monitor how does the curve evolve if we modify the size of the training set. Then let's compute the score ($r²$) for all of our trainings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9277a5c-b2b8-430c-b289-61bd432f10d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div>\n",
    "<img src=\"files/learning_curves.jpg\" width=\"75%\" align='center' source='https://www.dataquest.io/blog/learning-curves-machine-learning/'/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50033269-c5e6-48e1-9a1a-d9a6a754a9b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Understanding the curves\n",
    "\n",
    "### A good model\n",
    "\n",
    "As the training size increases :\n",
    "\n",
    " - The training score will decrease.\n",
    " - The test score will increase.\n",
    " - Most of the time the curves demonstrate convergence.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"files/learning_curves_score.png\" width=\"75%\" align='center' source='https://www.dataquest.io/blog/learning-curves-machine-learning/'/> </div>\n",
    "\n",
    "If the curves converge, it means that your model performs as well on the training set as it does on the test set. It does not perform worse on the test set, so there is no overfitting.\n",
    "\n",
    "But other scenarios can occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e81522-ea85-47fa-b39c-0ac65f9864e8",
   "metadata": {},
   "source": [
    "### High biais (underfitting)\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"files/high_biais.png\" width=\"75%\" align='center' source='https://www.dataquest.io/blog/learning-curves-machine-learning/'/> </div>\n",
    "\n",
    "Here the curves converge, so, just as before, there is no overfitting, but the model performs very poorly overall. Here we tried to use linear regression when clearly it is not a suitable model. So the model is underfitted, but it did not learn at all. There is therefore a strong bias, and we make a lot of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b8e56c-ac39-4e98-ad1c-3948b5dbbeb2",
   "metadata": {},
   "source": [
    "### High variance (overfitting)\n",
    "\n",
    "<div>\n",
    "<img src=\"files/high_variance.png\" width=\"75%\" align='center' source='https://www.dataquest.io/blog/learning-curves-machine-learning/'/> </div>\n",
    "\n",
    "In this other scenario, we observe that the training data has been perfectly predicted, but the score is low on the test data. There is therefore a significant gap between the two curves. We're dealing with an overfitting case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25d9c8d-4484-4ccf-83d5-03c7d6b71993",
   "metadata": {},
   "source": [
    "### Ideal curves\n",
    "\n",
    "<div>\n",
    "<img src=\"files/learning_curves_ideal_curves.png\" width=\"75%\" align='center' source='https://www.dataquest.io/blog/learning-curves-machine-learning/'/> </div>\n",
    "\n",
    "Ideal curves are therefore quite high (good score on both curves so no underfitting) and converge (no overfitting).\n",
    "\n",
    "### Too much data?\n",
    "\n",
    "These curves also indicate when it is not necessary to continue providing data to the model. By providing only what is strictly necessary, we shorten the duration of each model training, which allows us to save time and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097b6ccc-423b-4202-9bff-962215b5de02",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Learning curves on our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71f770e-bc5a-4db5-bcb9-f95ccc4ac8a4",
   "metadata": {},
   "source": [
    "## With linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e00300-b2bc-4548-b66b-1a423c85b30c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes = [25,50,75,100,250,500,750,1000,1150, 1400]\n",
    "\n",
    "# Get train scores (R2), train sizes, and validation scores using `learning_curve`\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator=LinearRegression(),\n",
    "                                                        X=X,\n",
    "                                                        y=y,\n",
    "                                                        train_sizes=train_sizes,\n",
    "                                                        cv=5)\n",
    "\n",
    "# Take the mean of cross-validated train scores and validation scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_scores_mean, label='Training score')\n",
    "plt.plot(train_sizes, test_scores_mean, label='Test score')\n",
    "plt.ylabel('r2 score', fontsize=14)\n",
    "plt.xlabel('Training set size', fontsize=14)\n",
    "plt.title('Learning curves', fontsize=18, y=1.03)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c48b740-8583-439a-b171-79e3019dfe41",
   "metadata": {},
   "source": [
    "This graph show that our curves have converged (the gap between them isn't that large), and the test score has plateaued. Even if we add more data, it won't increase the score.\n",
    "\n",
    "Also the score is below 0.7, so we can probably improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8772469f-44e0-446f-86ea-9b31feaeb9be",
   "metadata": {},
   "source": [
    "## With DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401bdd31-ad85-4659-9af0-4cfadeb7c35a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "train_sizes = [25,50,75,100,250,500,750,1000,1150, 1400]\n",
    "\n",
    "# Get train scores (R2), train sizes, and validation scores using `learning_curve`\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator=DecisionTreeRegressor(),\n",
    "                                                        X=X,\n",
    "                                                        y=y,\n",
    "                                                        train_sizes=train_sizes,\n",
    "                                                        cv=5)\n",
    "\n",
    "# Take the mean of cross-validated train scores and validation scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_scores_mean, label='Training score')\n",
    "plt.plot(train_sizes, test_scores_mean, label='Test score')\n",
    "plt.ylabel('r2 score', fontsize=14)\n",
    "plt.xlabel('Training set size', fontsize=14)\n",
    "plt.title('Learning curves', fontsize=18, y=1.03)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2154a7-685e-4b74-89c1-889da4d4bb37",
   "metadata": {},
   "source": [
    "Here the gap between the curves is bigger : the model perform very well on the train data set but doesn't behave as we wish on the test set : we're dealing with overfitting.\n",
    "\n",
    "The Test score curve keeps going up, meaning that if he wad more data, oure results would probably be better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682ea7cd-967b-49c0-8cbd-18e394560beb",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### What happens next ?\n",
    "\n",
    "The holdout method is here to help us improve our model but when we think we have done everything we could to improve our model (best split, best hyperparameters...), we can retrain the model but this time using the entire dataset to further improve accuracy. Then, we could test it on real data from a different dataset to see how it performs.\n",
    "\n",
    "Another possibility would be to use a different model and see if it performs better or worse."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
