{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2749c6e8-c030-4f4e-af9d-153c3325ebb3",
   "metadata": {},
   "source": [
    "# Cleaning and preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d080241-3ffd-40c1-b31c-1c3b4fbdf819",
   "metadata": {},
   "source": [
    "## Why do we need to prepare the data?\n",
    "\n",
    "Data is almost always \"dirty\", meaning we need to make sure it is suitable to machine learning models.\n",
    "\n",
    "Actually a huge part of the job of a data scientist is cleaning and manipulating the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5271f752-5c24-4e9f-b18e-e04d1729c53c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div>\n",
    "<img src=\"files/data_science_cleaning.png\" width=\"100%\" align='center' source='virgilus'> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3711618b-cf19-46ec-b10f-a141fbd477c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div>\n",
    "<img src=\"files/features_preparation.png\" width=\"100%\" align='center' source='https://www.orita.ai/blog/clean-customer-data-improves-data-stack'/> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0fcd52-3bf6-442b-821b-8135bd7a2d68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "cols = ['WallMat', # The material the walls are made of\n",
    "        'Alley', # Alley: Type of alley access to property. Grvl -> Gravel, Pave -> Paved, NA -> No alley access\n",
    "        'Pesos', # The price of the house in Pesos\n",
    "        'GrLivArea', # Above grade (ground) living area square feet\n",
    "        'BedroomAbvGr', # Bedrooms above grade (does NOT include basement bedrooms)\n",
    "        'KitchenAbvGr', # Kitchens above grade\n",
    "        'OverallCond', # Rates the overall condition of the house 10 is \"Very Excellent\" and 1 is \"Very Poor\"\n",
    "        'Street', # Type of road access to property Grvl -> Gravel, Pave -> Paved\n",
    "        'SalePrice', # Target ($)\n",
    "       ]\n",
    "\n",
    "df = pd.read_csv('data/iowa_housing.csv',\n",
    "                  usecols=cols\n",
    "                )\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bd3608-c453-4c64-98c6-fcb096c3ab0a",
   "metadata": {},
   "source": [
    "## Exploration (very quick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc25288-bd4a-49f3-b3d1-2b16b0c16390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac2d2ed-a7df-4162-ae90-39044f3c9143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c08016c-3c7b-4620-af4f-64e7bfc5911a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895990cc-8606-4a26-9d92-4f730becc1a4",
   "metadata": {},
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ad2509-8435-40b0-b20d-c0e42f3af6ba",
   "metadata": {},
   "source": [
    "### Duplicates\n",
    "\n",
    "When using some models, duplicates lines can lead to \"data leakage\". Indeed if a sample is both inside the train set and test set, then our test sample isn't really a test anymore. It's a good practice to remove the duplicates before selecting the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff52b7-9613-413d-959e-864904425e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea2e6cf-7bab-450e-8004-60d141b1b27c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates().reset_index(drop=True) # Let's create a new index and get rid of the \"Id\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa86374-81cf-489e-84ec-87a03bb18c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d927a-2e30-49d8-b5b7-25039d14d5aa",
   "metadata": {},
   "source": [
    "## Missing values\n",
    "\n",
    "- Missing values can be challenging to handle because there is no standardized way to represent them. Although since version 2 of Pandas, significant efforts have been made to standardize the representation of different types of missing values.\n",
    "\n",
    "- There are many reasons to get missing values (bug, bad measures, random events...) in a dataset.\n",
    "\n",
    "    - Sometimes we just want to get rid of them.\n",
    "    - Sometimes they actually provide useful informations about a phenomenon.\n",
    "    - Sometimes we want to replace them by the mean or the median of the Series. But we shouldn't do it if more than about 30% of our Series consists in missing values. Also, keep in mind it creates some noise in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf38b8-98ac-4c4f-b2d5-31c9f5e9223d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(df.isna().sum() / df.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df870f37-0f5a-4650-aa21-d6c6b8cfbfbe",
   "metadata": {},
   "source": [
    "### WallMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f885673-f61a-4f97-aa82-30e1728903f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns='WallMat') # Let's get rid of this empty column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3150650c-0d5d-4077-931a-bddaccc5f9a9",
   "metadata": {},
   "source": [
    "### Alley\n",
    "\n",
    "Let's take a look at the documentation :\n",
    "\n",
    "```\n",
    "Alley: Type of alley access to property\n",
    "\n",
    "       Grvl\tGravel\n",
    "       Pave\tPaved\n",
    "       NA \tNo alley access\n",
    "```\n",
    "\n",
    "So, an empty value is actually a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daf98b9-ab05-4d90-89f5-33864fdaf289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Alley'] = df['Alley'].fillna('NoAlley')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a997352-ecab-45ca-902c-827c252d4de7",
   "metadata": {},
   "source": [
    "### Pesos\n",
    "\n",
    "A very small portion of our dataset don't have \"Pesos\" values. One way to deal with this could be to replace the values with the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdf4c94-2af6-4e52-b4ab-df364454a9f8",
   "metadata": {},
   "source": [
    "#### Replacing values with a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2878f866-53fc-4236-9da9-89a739e66b41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Pesos'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f354a81-f43b-4144-979b-c040e24aec5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Pesos'].fillna(df['Pesos'].mean()) # not an 'inplace' method, so df has not been modified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad348409-e0f1-4fcf-bf0a-5378a204d5b7",
   "metadata": {},
   "source": [
    "#### Replacing using the \"SimpleImputer\" function from sklearn\n",
    "\n",
    "This is an \"estimator\", and it works pretty much the same as a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fb097c-aa8d-4aec-bfbb-762744c6103a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(df[['Pesos']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2611dd1-3d43-4fbb-b91e-0eef7bcefd31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imputer.statistics_ # median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a6d96a-663e-4135-af7a-50969bb38bb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imputer.transform(df[['Pesos']]) # Outputs the data transformed, missing values have been replaced by the median."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6552b9-9f06-4d19-b8d3-7d024b1ee768",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Replacing values with a custom function\n",
    "\n",
    "Let's have a look at the lines with missing values for the \"Pesos\" column. It's easy, we only have 10 of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b29b0c-62c3-49cd-9963-3f4f4872972b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[df['Pesos'].isna()] # We still have the SalePrice in $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee9bcc-285a-4053-bdf6-5251cb56a54c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df.loc[~df['Pesos'].isna()] # To visualize the lines with a value\n",
    "#df.loc[df['Pesos'].notna()] # Another method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218e6d68-e85f-4f14-9e01-2d93d0cab8d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pesos_missing_values_index = df.loc[df['Pesos'].isna()].index # Let's save the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eec9d3-3f4d-4292-857e-52a94a4f18e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(df['Pesos'] / df['SalePrice'])#.unique() # Let's check how much is one peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2897f9-dc02-4243-b68e-0df81a0082b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Pesos'] = df['Pesos'].fillna(df['SalePrice'] * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed59c612-edb7-4956-9f3a-e9de3f093b0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[pesos_missing_values_index] # Making sure everything is right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb0f19f-8028-41f4-8a32-d025fad236c1",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "Just like we saw previously with the missing values, outliers can also occur in a dataset. Maybe the user entered a wrong number, maybe the sensor had a bug, maybe a value has been wrongly encoded.\n",
    "\n",
    "Having outliers can affect your model performances. It also affects statistics such as the mean or the standard variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b0b620-d7d7-4652-9f34-f11820818f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.int8(89) + np.int8(93) # Example of bad encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83adb6fa-3f02-4dad-83cf-6708ac444205",
   "metadata": {},
   "source": [
    "### Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74de93a0-7a94-4d1f-ba56-e55863c1358b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[['GrLivArea']].boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8631dd23-a638-4dc5-93cd-628af4c75461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[['GrLivArea']].min() # Impossible value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24346caf-f1ef-4994-8756-c08af993da1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['GrLivArea'].argmin() # Get the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6587f4-0aa2-4fd4-b540-d0f3b50c5fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdadd8d-1dc9-4e5b-afad-e97399075d35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(df['GrLivArea'] < 10).sum() # Check how many rows are concerned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb43619c-21d8-4edc-bf5f-fbd6f230cbc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[(df['GrLivArea'] < 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde368af-b0f6-4b79-ac88-7e55112df7f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df = df.drop(index=10) # Alternative method\n",
    "df = df.loc[(df['GrLivArea'] > 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9bb50-46db-418b-9368-8ffa72aab10c",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d199761-38b0-4a68-a04f-c8ff95f89a8a",
   "metadata": {},
   "source": [
    "### Why scaling?\n",
    "\n",
    "- Features with large magnitudes can incorrectly outweight features of small magnitudes.\n",
    "- Scaling to smaller magnitudes improves computationnal efficiency.\n",
    "- Increases interpretability of feature coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8ef407-294c-45b1-845c-7c7168be9b53",
   "metadata": {},
   "source": [
    "### Scaling continuous values\n",
    "\n",
    "We're going to transform continuois features into a common, smaller range.\n",
    "Beware, not every numeric value is a continuous value (departement number, id, classes represented with an int, ordinal values...)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09fe373-ad5b-44b9-ac8e-d40651eafae2",
   "metadata": {},
   "source": [
    "### Standardizing\n",
    "\n",
    "With ```sklearn.preprocessing.StandardScaler```, we can transform a feature so that is has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "<font size=\"6\">\n",
    "$z = \\frac{{(x - \\text{mean})}}{{\\text{std}}}$\n",
    "</font>\n",
    "\n",
    "In most large data sets (assuming a normal distribution of data) :\n",
    "\n",
    "- 99.7% of values lie between -3 and 3 standard deviations,\n",
    "- 95% between -2 and 2 standard deviations\n",
    "- 68% between -1 and 1 standard deviations.\n",
    "\n",
    "If our distribution is bell-shaped, standard scaling is probably the way to go but :\n",
    "\n",
    "- It does not ensure an exact common range.\n",
    "- It's sensitive to outliers.\n",
    "- It can distort relative distances between feature values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc456d7-8957-4f2b-8c41-4b5d0653281d",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div>\n",
    "<img src=\"files/standardisation.png\" width=\"55%\" align='center'> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b697486-0c9a-4d36-aedf-c286842f6e06",
   "metadata": {},
   "source": [
    "### Normalizing with Min / Max scaling\n",
    "\n",
    "An other way to proceed is normalizing the data with ```sklearn.preprocessing.MinMaxScaler```. All values will then be compressed in a fix range from 0 to 1 (by default).\n",
    "\n",
    "<font size=\"6\">\n",
    "$X' = \\frac{{(X - X_{min})}}{{X_{max} - X_{min}}}$\n",
    "</font>\n",
    "\n",
    "The Min-Max Scaling is efficient regardless of distribution but :\n",
    "\n",
    "- It doesn't reduce the affect of outliers.\n",
    "- It doesn't correct the skewness of a distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732030b0-81e9-4e98-9e60-9f14eeb5bcce",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"files/standardized_and_normalized_data.png\" width=\"85%\" align='center'> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed82eed-223e-4fba-95d5-680effbb05fa",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271cb823-badf-4500-8d64-b74a38d2551d",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b7bcb-5540-458a-ad91-0879e9d016be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Pesos'].plot(kind='hist', bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae61908-1f2d-4ab3-b783-28f35344814f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standardscaler = StandardScaler().fit(df[['Pesos']])\n",
    "df['Pesos'] = standardscaler.transform(df[['Pesos']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde941ba-96d9-498f-911e-5f1a4dbc72d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "standardscaler.mean_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb31e1c9-8ae8-43ae-8785-b750fb91f943",
   "metadata": {},
   "source": [
    "### Min / Max Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba62323-02bb-48ad-b985-6ba355406303",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minmaxscaler = MinMaxScaler().fit(df[['GrLivArea']])\n",
    "df['GrLivArea'] = minmaxscaler.transform(df[['GrLivArea']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2b6d43-33cb-4dac-bd8c-9e4e371d3799",
   "metadata": {},
   "source": [
    "Most of the time, it is ok to mix different types of scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db32d716-836c-4bdc-a990-9b914dab4c61",
   "metadata": {},
   "source": [
    "## Dataset Balancing\n",
    "\n",
    "Most of the time in a dataset, each class is imbalanced.\n",
    "\n",
    "- Maybe you have more people healthy rather than sick.\n",
    "- Maybe you have more men than woman\n",
    "- Etc...\n",
    "\n",
    "Our model will perform better on the classes it has encountered the most. We can balance data whether if it's X or y (target)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaabd99-4838-4505-973d-894c5561b19c",
   "metadata": {},
   "source": [
    "### Balancing strategies\n",
    "\n",
    "- Oversampling of minority class\n",
    "- Alternatively, computation of new instances for the minority class\n",
    "- Undersampling of majority class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116ec5a2-9518-4523-b784-e36dba3b2b40",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div>\n",
    "<img src=\"files/undersampling_oversampling.png\" width=\"85%\" align='center' source='https://www.kaggle.com/code/rafjaa/resampling-strategies-for-imbalanced-datasets#t1'> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d9794-f388-4f14-8a42-f94a707f17cb",
   "metadata": {},
   "source": [
    "Which one is better ?\n",
    "\n",
    "- **Oversampling** : We're going to duplicates samples. But remember, we should do it only after we've done our train / test split. Otherwise we might have data leakage.\n",
    "\n",
    "- **Undersampling** : In that case we don't have to generate fake data but we're losing some real data that might have been useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b978bc44-feef-4d87-aea5-fa0dc7381d0e",
   "metadata": {},
   "source": [
    "## Synthetic Minority Oversampling TEchnique (SMOTE)\n",
    "\n",
    "> SMOTE is an oversampling algorithm that generates new minority instances from existing minority instances - based on linear combinations of existing points.\n",
    "\n",
    "So we can add new fake data points that look real."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39bb26c-190c-4f9c-a0aa-9db9327280b1",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"files/SMOTE_1.png\" width=\"85%\" align='center' source='@rikunert'> </div>\n",
    "\n",
    "<div>\n",
    "<img src=\"files/SMOTE_2.png\" width=\"85%\" align='center' source='@rikunert'> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638749b8-0091-4e31-932c-fa18c6f5bcca",
   "metadata": {},
   "source": [
    "### Usage warning\n",
    "\n",
    "This technique must be applied only on the training set. The test set exists only in order to provide a score so it must be real data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7a9497-6da5-4a4d-a3c8-999bec026d66",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "\n",
    "Most of the models don't understand any other value than numeric values. Encoding allows us to transform non-numerical data to an equivalent form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130bb05a-f778-4c5d-b101-4f52e9cf7f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Alley'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268df429-0c31-4384-8da5-aa2cfb5c8019",
   "metadata": {},
   "source": [
    "### Integer Encoding\n",
    "\n",
    "It is the process of replacing a string (usually a class/category) with a number. For instance :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a93272-fe95-446f-8213-cdbc17d2f91e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fruit_df = pd.DataFrame({'Fruit': ['Apple', 'Banana', 'Apple', 'Grape', 'Banana', 'Mango']})\n",
    "fruit_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad6ef87-2539-4142-be40-dd05c5086fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's create a mapping dictionary to encode the Fruit column\n",
    "\n",
    "mapping_dictionary = {'Apple' : 0, 'Banana': 1, 'Grape': 3, 'Mango': 4}\n",
    "fruit_df['Integer_encoding'] = fruit_df['Fruit'].map(mapping_dictionary)\n",
    "fruit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431740bb-5a27-466d-a6ea-a2f944104640",
   "metadata": {},
   "source": [
    "### Ordinal encoding\n",
    "\n",
    "We can also encode ordinal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63234e90-a56d-4c1e-b62f-2ecddc437561",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fruit_df = pd.DataFrame({'Fruit': ['Apple', 'Banana', 'Apple', 'Grape', 'Banana', 'Mango'],\n",
    "                         'Taste': ['Good', 'Bad', 'Average', 'Good', 'Good', 'Bad']\n",
    "                        })\n",
    "fruit_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14120050-3a83-426b-8ce9-c421656f861f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "ordinal_encoder.fit(fruit_df[[\"Taste\"]])\n",
    "\n",
    "fruit_df[\"Taste_encoded\"] = ordinal_encoder.transform(fruit_df[[\"Taste\"]])\n",
    "\n",
    "fruit_df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693c987f-f976-4bfc-b7e8-dd1778e498ce",
   "metadata": {},
   "source": [
    "But this doesn't look right. It's because we didn't specify any order so our function used the alphabetical order. Let's fix that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27c905b-550a-4449-99c8-80be56a7a42e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder(categories=[[\"Bad\",\"Average\",\"Good\"]]) # Adding a new parameter\n",
    "ordinal_encoder.fit(fruit_df[[\"Taste\"]])\n",
    "\n",
    "fruit_df[\"Taste_encoded\"] = ordinal_encoder.transform(fruit_df[[\"Taste\"]])\n",
    "\n",
    "fruit_df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2060341c-c7db-4676-a217-4abf07029e75",
   "metadata": {
    "tags": []
   },
   "source": [
    "### One hot-encoding\n",
    "\n",
    "It will create a binary column for each possible category.\n",
    "\n",
    "#### With sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2def66-a598-43aa-950f-77c7294b5bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "fruit_df = pd.DataFrame({'Fruit': ['Apple', 'Banana', 'Apple', 'Grape', 'Banana', 'Mango']})\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False).fit(fruit_df[['Fruit']]) # sparse_output = False so we don't have to deal with a scipy compressed matrix object\n",
    "\n",
    "print(f\"The categories detected by the OneHotEncoder are {ohe.categories_}\")\n",
    "print(f\"The column names for the encoded values are {ohe.get_feature_names_out()} \\n\")\n",
    "print(ohe.transform(fruit_df[['Fruit']]))\n",
    "\n",
    "fruit_df[ohe.get_feature_names_out()] = ohe.transform(fruit_df[['Fruit']])\n",
    "fruit_df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d211569-fe6d-4554-a799-54b605c0824d",
   "metadata": {},
   "source": [
    "#### With Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749d1407-064b-4ae4-bc20-20aef971b9f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fruit_df = pd.DataFrame({'Fruit': ['Apple', 'Banana', 'Apple', 'Grape', 'Banana', 'Mango']})\n",
    "\n",
    "pd.get_dummies(fruit_df['Fruit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21d2287-fdc6-4614-8b7b-e6c44a225e6a",
   "metadata": {},
   "source": [
    "#### Difference between One Hot Encoding and Dummy variables\n",
    "\n",
    "- One Hot Encoding usually means you're encoding all your categories.\n",
    "- Dummy variables usually means all your categories minus one.\n",
    "\n",
    "Indeed some models work better if instead of $k$ categories, you have $k-1$ categories. In that case you can set the parameter ```drop_first``` to ```True``` with Pandas, or the parameter ```drop``` in scikitlearn which has different options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c667c95-cf28-4670-8cfd-102eabad504d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fruit_df = pd.DataFrame({'Fruit': ['Apple', 'Banana', 'Apple', 'Grape', 'Banana', 'Mango']})\n",
    "\n",
    "pd.get_dummies(fruit_df['Fruit'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0418d1-3433-4bb1-a46c-fd3329cce7bb",
   "metadata": {},
   "source": [
    "### Encoding 'Alley'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd530f2d-d429-4ea3-a3b5-a775929dd909",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Alley'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409823c7-ffc3-4009-b169-e2844352a652",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False, drop=['NoAlley']).fit(df[['Alley']])\n",
    "\n",
    "print(ohe.categories_) # Basically the fit is just performing a .unique()\n",
    "\n",
    "df[ohe.get_feature_names_out()] = ohe.transform(df[['Alley']])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78071b80-fa69-46cd-9571-b430d435570b",
   "metadata": {},
   "source": [
    "## Discretization\n",
    "\n",
    "\"Discretization\" refers to the process of converting or partitioning continuous attributes, features or variables to discretized or nominal attributes/features/variables/intervals.\n",
    "\n",
    "It could help us if we wanted to predict a class and not a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b966b2b-6a61-4176-85b3-c7160990cb36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['SalePriceClass'] = pd.cut(x=df['SalePrice'],\n",
    "                              bins=[df['SalePrice'].min() - 1,\n",
    "                                    df['SalePrice'].mean(),\n",
    "                                    df['SalePrice'].max() + 1], \n",
    "                              labels=['cheap', 'expensive'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399da976-acad-4458-b2d7-20fa33981cb3",
   "metadata": {},
   "source": [
    "## Feature Creation\n",
    "\n",
    "If we understand the dataset, we can create new features in order to improve the performance of the data.\n",
    "Such as :\n",
    "\n",
    "- Creating the body mass index ( $height / weight²$)\n",
    "- Compute the time between two events.\n",
    "- Categorize dates as weekday, weekend, holidays etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236a6efc-d466-43fe-ad45-ae8c4f7f3175",
   "metadata": {},
   "source": [
    "##  Feature Selection\n",
    "\n",
    "How can we select our features? How do we know which column is useful?\n",
    "\n",
    "Selecting only useful columns makes the training process faster, it reduces the number of dimensions and thus reduce the complexity of the model.\n",
    "\n",
    "### The curse of Dimensionality\n",
    "\n",
    "More data doesn't mean that the model will be able to better generalize. Actually it can lower the model performances.\n",
    "\n",
    "<div>\n",
    "<img src=\"files/curse_of_dimensionality.png\" width=\"45%\" align='center' source=\"https://builtin.com/data-science/curse-dimensionality\"><br></div>\n",
    "\n",
    "As the number of features or dimensions grows, the amount of data we need to generalize accurately grows exponentially. If we start from one feature and add a new one, this will cause an increase in dimension space to $4 * 4 = 16$. And then $4 * 4 * 4 = 64$, and so on. So as the dimensions keep on increasing, dimensions space increases exponentially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01924a0-f895-45e5-af0c-b6a58a39d2d4",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"files/curse_of_dimensionality_2.png\" width=\"65%\" align='center' source=\"https://builtin.com/data-science/curse-dimensionality\"><br></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3a99d0-d81a-4740-8850-3de49adab263",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Feature correlation\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(df.corr(numeric_only=True),\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37a8723-9622-4142-baf6-932e0c3793a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coef_df = df.corr(numeric_only=True).stack().reset_index().rename(columns={'level_0': 'feature_1', 'level_1': 'feature_2', 0:'coef'})\n",
    "coef_df['coef'] = np.abs(coef_df['coef']) \n",
    "coef_df.loc[coef_df['feature_1'] != coef_df['feature_2']].sort_values(by='coef', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2992c9d-6761-4433-987f-5f646816987c",
   "metadata": {},
   "source": [
    "### Dropping unwanted columns\n",
    "\n",
    "Obviously here we have a huge data leak : the 'Pesos' column is the same than the target feature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f28c8d9-2952-415b-886e-a8ac22a72bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns='Pesos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc7ccad-2c18-49de-8dfe-98ec210417cd",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Let's try to predict a class rather than a number using logistic regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a75dc29-75fe-402c-bbc7-628acba5cbdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Encoding the target\n",
    "target_encoder = LabelEncoder().fit(df['SalePriceClass']) \n",
    "y = target_encoder.transform(df['SalePriceClass'])\n",
    "\n",
    "# Defining the features\n",
    "X = df.drop(columns=['SalePrice', 'SalePriceClass', 'Street', 'Alley'])\n",
    "\n",
    "# Scaling numerical features\n",
    "# Notice that we already normalize GrLivArea\n",
    "minmax_scaler = MinMaxScaler()\n",
    "X[[\"BedroomAbvGr\",\"KitchenAbvGr\",\"OverallCond\"]] = minmax_scaler.fit_transform(X[[\"BedroomAbvGr\",\"KitchenAbvGr\",\"OverallCond\"]])\n",
    "\n",
    "# Instantiate model\n",
    "log_reg = LogisticRegression(max_iter=1000) \n",
    "\n",
    "# Scoring on multiple folds aka Cross Validation, so no need to\n",
    "scores = cross_val_score(log_reg, X, y, cv=10)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7ebc35-00d6-47a1-8740-4e0a18bb1246",
   "metadata": {},
   "source": [
    "## Data leakage !\n",
    "\n",
    "It seems like a good score. But actually we made two mistakes of data leakage\n",
    "\n",
    "1. During the scaling stage (Standard Scaler and Normalization), we applied our transformations onto our entire dataset.\n",
    "\n",
    "    So we computed the mean and the standard variation for both our train and test sets, and a little bit of information has leaked into our test set (inside the cross validation).\n",
    "\n",
    "1. During the encoding stage, we used the OneHotEncoder for the column 'Alley' on our entire dataset. So it knows all possible variables.\n",
    "\n",
    "1. In the last cell we used our MinMaxScaler on our entire columns.\n",
    "\n",
    "1. We removed the outliers from our entire dataset, but in real life maybe our test set can contain outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21561e5c-9f51-46f7-b19e-6ea18f080c8e",
   "metadata": {},
   "source": [
    "## Feature permutation\n",
    "\n",
    "\"Permutation feature importance is a model inspection technique that measures the contribution of each feature to a fitted model’s statistical performance on a given tabular dataset. This technique involves randomly shuffling the values of a single feature and observing the resulting degradation of the model’s score. By breaking the relationship between the feature and the target, we determine how much the model relies on such particular feature.\" [From the sklearn doc.](https://scikit-learn.org/stable/modules/permutation_importance.html)\n",
    "\n",
    "As this method needs to train the model a great number of times, it can only work if the model is simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b6ba00-6fb5-4e32-aacf-bcc48b9b2ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Fit model\n",
    "log_model = LogisticRegression().fit(X, y) \n",
    "\n",
    "# Performs Permutation\n",
    "permutation_score = permutation_importance(log_model, X, y, n_repeats=10) \n",
    "\n",
    "# Unstack results showing the decrease in performance after shuffling features\n",
    "importance_df = pd.DataFrame(np.vstack((X.columns,\n",
    "                                        permutation_score.importances_mean)).T) \n",
    "importance_df.columns=['feature','score decrease']\n",
    "\n",
    "# Show the important features\n",
    "importance_df.sort_values(by=\"score decrease\", ascending = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6129e0-f1e4-452b-af6c-7cee59e00ba1",
   "metadata": {},
   "source": [
    "Then we can try to retrain the model removing the useless columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae98ce6-f8c7-491d-9913-d54d0a9517be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selecting the strongest features\n",
    "strongest_features = X.drop(columns=['Alley_Pave', 'OverallCond'])\n",
    "\n",
    "# Re-instantiating a Logistic Regression\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Average accuracy of the cross-validated model\n",
    "np.mean(cross_val_score(log_reg, strongest_features, y, cv=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
