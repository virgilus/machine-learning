{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67884f0a-3bb0-4141-a808-b98e01ac8019",
   "metadata": {},
   "source": [
    "# Fitting\n",
    "\n",
    "We're going to take a closer look at the parameters and how we can enhance this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e87fbe7-ea87-4a67-822e-1e9eb0e2de6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What happens when we fit the model\n",
    "\n",
    "Let's start with a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ac8b2a-79b6-4e58-8c24-c1756aacc3c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({'weight': [0.7, 2.4, 2.8],\n",
    "                   'height': [1.5, 1.8, 3.2]})\n",
    "\n",
    "X = df[['weight']].values\n",
    "y = df['height'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015988f1-9887-4108-bf10-a4e0eda403d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "size=[0, 4]\n",
    "\n",
    "def draw_chart(X=X, y=y, size=size):\n",
    "\n",
    "    plt.scatter(X, y)\n",
    "    plt.xlabel('weight')\n",
    "    plt.ylabel('height')\n",
    "    plt.axis(size*2)\n",
    "    \n",
    "draw_chart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a69681-18ba-4a74-b5bd-14440492c3e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc70c216-a878-4a69-ae6b-3abb9955c779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a coefficient\n",
    "model.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb3c17b-33b2-4abc-b7a0-2391018081df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# b coefficient\n",
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681d2d88-8068-47ed-ad91-6de6d292ad1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "draw_chart()\n",
    "plt.plot(size, model.predict(np.array(size).reshape(-1, 1)), color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dde630a-d5f5-4495-96ec-b7e0c4bff6c5",
   "metadata": {},
   "source": [
    "## Model prediction in a equation\n",
    "\n",
    "What happens during `.fit()`? Any model can be expressed as: \n",
    "\n",
    "$y = h(X, \\beta) + error$\n",
    "\n",
    "- $h$ is called our hypothesis function\n",
    "- $X$ is our input (features)\n",
    "- $\\beta$ is the parameters of our model\n",
    "- $error$ is the difference between y true ($y$) and y pred ($\\hat{y}$).\n",
    "- $h(X, \\beta)$ is called our prediction ($\\hat{y}$)\n",
    "\n",
    "## For a Linear Regression\n",
    "\n",
    "For a linear regression : $h(X, \\beta) = \\beta_0 + \\beta_1 X_1$, where $\\beta_0$ is the interpect and  $\\beta_0$ our coef ($a$).\n",
    "\n",
    "When we run a ```.fit()```, the algorithm tries to find the best parameters $\\beta_0$ and  $\\beta_0$ in order to minimize the $error$.\n",
    "\n",
    "## L2 Loss Function\n",
    "\n",
    "L2 Loss Function is used to minimize the error which is the sum of the all the squared differences between the true value and the predicted value.\n",
    "\n",
    "```.fit()``` minimizes $L(error)$\n",
    "$L_{OLS} = \\lVert error \\rVert^2 = \\lVert y - \\beta_0 - \\beta_1 X_1 \\rVert^2$\n",
    "\n",
    "(OLS = Ordinary Least Squares, sometimes written LS = Least Square Errors)\n",
    "\n",
    "We often write: $\\beta = \\text{argmin}_\\beta L(\\beta, X, y, h)$\n",
    "\n",
    "**Note:** You also have the L1 Loss (also called LAD = Least Absolute Deviations), less sensitive to outliers, but less used:\n",
    "$L_{L1} = \\lVert error \\rVert = \\lVert y - \\beta_0 - \\beta_1 X_1 \\rVert$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595f7962-55bc-4594-85a2-9446a25c0ad1",
   "metadata": {},
   "source": [
    "## Solvers\n",
    "\n",
    "There are different \"solvers\" you can use to minimize $L(\\beta)$ :\n",
    "- Exact mathematical resolution (matrix inversion, but often too complex)\n",
    "- Iteratives approaches. We'll assign $\\beta_0$ and $\\beta_1$ random values and we're going to minimize the errors through multiples iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616b99b-4c29-4906-873b-e796a068531e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LogisticRegression(solver='newton-cg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edaa16b-4fe6-4ff5-808d-8782ef47862e",
   "metadata": {},
   "source": [
    "Imagine that we already know the value of the ideal slope, $\\beta_1 = 0.64$, and need to find the optimal  $\\beta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d291a8a6-8d65-4437-9277-ca3a03b20abd",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div>\n",
    "<img src=\"files/height_weight_SSR_intercept.png\" width=\"70%\" align='center' source=\"https://github.com/MoinDalvs/Gradient_Descent_For_beginners\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7e1c0a-bd01-4020-8630-b24965c3b613",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Let's randomly initialize an intercept, say at 0.\n",
    "1. Now, let's compute the loss at that intercept value; here, the loss is the Sum of Squared Residuals (**SSR**).\n",
    "1. Change the intercept and repeat the process until we find the smallest loss.\n",
    "\n",
    "If we look at the Loss Function, we see that it has a convex shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55974911-8ba4-425d-b183-c6dd5ceec128",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"files/solver_ssr_1.png\" width=\"70%\" align='center' source=\"https://github.com/MoinDalvs/Gradient_Descent_For_beginners\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2584e1ae-7d5f-44f7-8447-b8a81127adf6",
   "metadata": {},
   "source": [
    "**Problems**:\n",
    "\n",
    "- We could miss the exact minimum if our steps are too large\n",
    "- We don't know the best  $\\beta_1$ to start with.\n",
    "\n",
    "So let's discover the most basic but very powerful iterative method: the **Gradient Descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad750ed7-97b8-4ec4-b83f-a344e3631b14",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "### 1D Descent Step-by-Step\n",
    "\n",
    "- Uses the **slope** (gradient) of the Loss Function as an indicator.\n",
    "- As the slope approaches zero, the loss approaches its minimum.\n",
    "\n",
    "$\\frac{\\delta Loss Function}{\\delta Parameter}$\n",
    "\n",
    "The slope is equal to the partial derivative of the Loss Function with respect to the parameter of interest.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"files/gradient_descent_1.png\" width=\"70%\" align='center' source=\"https://github.com/MoinDalvs/Gradient_Descent_For_beginners\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5923a268-b208-44fe-931a-2112805fdb28",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Step One**\n",
    "\n",
    "Initialize a random parameter value, say $\\beta = 0$\n",
    " \n",
    "Calculate the derivative of the Loss Function at that point : $\\frac{\\delta SSR}{\\delta \\beta_0}$\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"files/gradient_descent_2.png\" width=\"40%\" align='center' source=\"https://github.com/MoinDalvs/Gradient_Descent_For_beginners\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2752c1-3064-4efe-bc7d-10746d80e537",
   "metadata": {},
   "source": [
    "**Step Two**\n",
    "\n",
    "Move in the opposite direction of the derivative by one step.\n",
    "\n",
    "<div>\n",
    "<img src=\"files/gradient_descent_3.png\" width=\"80%\" align='center' source=\"https://github.com/MoinDalvs/Gradient_Descent_For_beginners\" />\n",
    "</div>\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- The step size is proportional to the derivative's value\n",
    "- It moves according to a chosen Learning Rate = $\\eta$\n",
    "\n",
    "$$\\beta^{(1)}_0 = 0 - \\eta \\frac{\\partial L}{\\partial \\beta_0} (0)$$\n",
    "\n",
    "The $(1)$ above $\\beta$ meaning its our first \"epoch\" (an iteration). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714029c3-363d-4fb9-a507-e121a361815e",
   "metadata": {
    "tags": []
   },
   "source": [
    "The updated intercept value is plugged back into the derivative of the Loss Function, and we repeat the process.\n",
    "\n",
    "<div>\n",
    "<img src=\"files/gradient_descent_4.png\" width=\"80%\" align='center' source=\"https://github.com/MoinDalvs/Gradient_Descent_For_beginners\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263d5623-7c27-4167-9b11-63178e59f056",
   "metadata": {
    "tags": []
   },
   "source": [
    "As the loss approaches its minimum, the derivative gets smaller, and so do the steps.\n",
    "<div>\n",
    "<img src=\"files/gradient_descent_5.png\" width=\"80%\" align='center' source=\"https://github.com/MoinDalvs/Gradient_Descent_For_beginners\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d39054-6a82-47bd-a0e1-fe59f1f93bcc",
   "metadata": {
    "tags": []
   },
   "source": [
    "This makes the Gradient Descent computationally efficient. It does few calculations far away from the minimum, and more calculations as it approaches the minimum of the Loss Function.\n",
    "\n",
    "<div>\n",
    "<img src=\"files/gradient_descent_6.png\" width=\"80%\" align='center' source=\"https://github.com/MoinDalvs/Gradient_Descent_For_beginners\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ca83a8-d4c8-431d-a06f-8649be374cfc",
   "metadata": {},
   "source": [
    "**When does it stop?**\n",
    "The Gradient Descent algorithm can have different stopping criteria:\n",
    "\n",
    "- Minimum Step Size (e.g. 0.001). When the step size is smaller than this threshold, the Gradient Descent has converged, and the corresponding intercept is the optimal value\n",
    "- Maximum Number of steps (e.g. 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b8c1c-cbbd-416b-a94e-fe454f471d66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df['weight']\n",
    "y = df['height']\n",
    "\n",
    "b1 = 0.64\n",
    "eta = 0.1 # Learning Rate\n",
    "\n",
    "# Hypothesis function h\n",
    "def h(X, b0):\n",
    "    return b0 + b1 * X\n",
    "\n",
    "# Initialize intercept at 0 for this example\n",
    "b0_epoch0 = 0\n",
    "\n",
    "# L(b0_epoch_0)\n",
    "np.sum((y - h(X, b0_epoch0)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5118247f-70b1-4001-adcd-85b0f257f5c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: compute the derivative of the Loss Function at b0_epoch_0\n",
    "derivative = np.sum(-2 * (y - h(X, b0_epoch0)))\n",
    "derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ddd12-1303-4ddf-a7d3-1c8691ee3699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: update the intercept\n",
    "b0_epoch1 = b0_epoch0 - (eta * derivative)\n",
    "b0_epoch1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17117e13-467c-418b-9dc1-6d43278863c5",
   "metadata": {},
   "source": [
    "We've just done **one epoch**! Let's do an other one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c1d546-1366-46d8-9007-a15e860bc5c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step1: compute the new derivative at b0_epoch1\n",
    "derivative = np.sum(-2 * (y - h(X, b0_epoch1)))\n",
    "\n",
    "# Step2: update the previously updated intercept\n",
    "b0_epoch2 = b0_epoch1 - eta * derivative\n",
    "b0_epoch2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f62607-f8d9-43ea-b77e-f10bf251ad17",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div>\n",
    "<img src=\"files/gradient_descent_9.png\" width=\"40%\" align='center' source=\"https://github.com/MoinDalvs/Gradient_Descent_For_beginners\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecf7451-7186-4b71-916d-442d1c600375",
   "metadata": {},
   "source": [
    "Keep going until it converges to the minimum!\n",
    "\n",
    "You can also visualize it this way:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f05c2-5aac-4c31-9433-eac9af82e6c5",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"files/gradient_descent_7.webp\" width=\"55%\" align='center' source=\"https://ucanalytics.com/blogs/intuitive-machine-learning-gradient-descent-simplified/\" />\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "This is called the \"energy landscape\" or the \"potential energy surface\" of the Loss Function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6c1a36-3e7f-459f-b495-14b6750018ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "Or like this in more complex problems:\n",
    "\n",
    "<div>\n",
    "<img src=\"files/gradient_descent_8.webp\" width=\"60%\" align='center' source=\"https://towardsdatascience.com/its-only-natural-an-excessively-deep-dive-into-natural-gradient-optimization-75d464b89dbb\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6977df-fbff-4c20-923c-919f473344d5",
   "metadata": {},
   "source": [
    "## What is the effect of learning rate $\\eta$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3318db-4038-40a0-a29b-bceb8bf42def",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"files/gradient_descent_10.png\" width=\"80%\" align='center' source=\"https://github.com/MoinDalvs/Gradient_Descent_For_beginners\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38699df-0ccd-4134-9fcf-f32f71d975ad",
   "metadata": {},
   "source": [
    "**Small Learning Rate**\n",
    "\n",
    "- A shorter path to the minimum.\n",
    "- Requires more epochs.\n",
    "- May get stuck at local minima.\n",
    "\n",
    "**Large Learning Rate**\n",
    "\n",
    "- Requires fewer epochs.\n",
    "- May never converge!\n",
    "\n",
    "Also the Gradient Descent algorithm always converges faster when **features are scaled**!\n",
    "\n",
    "> \"Without feature scaling, gradient descent will require a lot more steps to reach the minima. In other words, gradient descent will take a lot of time to converge thus increasing the model training time.\" [source](https://medium.com/analytics-vidhya/why-gradient-descent-doesnt-converge-with-unscaled-features-8b7ed0c8cab6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed47670b-fc7c-4ed0-99be-46c6e72d6c06",
   "metadata": {},
   "source": [
    "## Other solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd64714-2ac6-4b1b-8cec-d8ddf5014668",
   "metadata": {},
   "source": [
    "Gradient Descent is computationally expensive on big datasets, couldn't we use less than all observations to compute an \"approximate loss\"?\n",
    "\n",
    "- **Mini-Batch Gradient Descent**: At each iteration compute an approximate loss on a mini batch of your data points.\n",
    "- **Stochastic Gradient Descent (SGD)**: A very common optimisation, that using random mini batch or single data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ed263e-ee7d-41c0-90b1-b8357fcc0d88",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"files/gradient_descent_11.png\" width=\"70%\" align='center' source=\"https://github.com/MoinDalvs/Gradient_Descent_For_beginners\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854bc26a-7efe-4db1-9f5b-bc82dc098dc6",
   "metadata": {},
   "source": [
    "**Pros**\n",
    "\n",
    "- SGD is faster for very large datasets.\n",
    "- Jumps out of local minima!\n",
    "- Greatly reduces RAM load (useful when you do Deep Learning).\n",
    "\n",
    "**Cons**\n",
    "\n",
    "- Needs more epochs.\n",
    "- Never exactly converges (careful when to stop).\n",
    "- Maybe slower for small $n$ datasets with many features $p$.\n",
    " \n",
    "To Use when:\n",
    "\n",
    "- The number of observations in your dataset has 6 digits or more.\n",
    "- You want to get \"un-stuck\" from a local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601adf10-561e-45a9-bea0-28f5ba832ceb",
   "metadata": {},
   "source": [
    "### Sklearn SGDRegressor and SGDClassifier\n",
    "\n",
    "- SGDRegressor is a Linear Model (Linear Regression) that uses the Stochastic Gradient Descent as a solver to minimize its Loss Function (MSE)\n",
    "- SGDClassifier is a Linear Model (Logistic Regression) that uses the Stochastic Gradient Descent as a solver to minimize its Loss Function (Log Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe959c35-d232-4726-9c0d-4c0ce772a9e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor, LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression() # OLS solved by matrix inversion (SVD method)\n",
    "lin_reg_sgd = SGDRegressor(loss='squared_error') # OLS solved by SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b2206a-54dc-4f36-ab16-1d6786bc3079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Create a \"fake problem\" to solve\n",
    "X, y = make_regression(n_samples=10_000, n_features=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d86f4-e181-4711-ad1a-789cea58d939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lin_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f5500-d3ff-471c-8bf3-a07439157268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lin_reg_sgd.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a8e210-54fa-4158-9652-7be63d9e6e78",
   "metadata": {},
   "source": [
    "# Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beaea4a-8af4-478a-b95d-e69a2288f152",
   "metadata": {},
   "source": [
    "For a given model, we have:\n",
    "\n",
    "- **Parameters** (computing automatically during the fitting process by minimizing $L(\\beta)$).\n",
    "\n",
    "- **Hyperparameters** (Loss function, loss parameters, solver, model specifities etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a60c92c-8f90-4169-b481-02fad8bf6040",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Improving model performances\n",
    "\n",
    "\n",
    "### Solutions for Overfitting\n",
    "\n",
    "In Machine Learning, most of the time we're dealing with overfitting. On order to reduce overfitting and allow the model to generalize better we can :\n",
    "\n",
    "- Get more observations (data)\n",
    "- Feature selection (manual or automated)\n",
    "- Dimensionality reduction (Unsupervised Learning)\n",
    "- Early stopping (Deep Learning)\n",
    "- **Regularization** of your Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d945ea33-3a5f-4b0c-abcf-5ef99f138b3f",
   "metadata": {},
   "source": [
    "## Model Complexity\n",
    "\n",
    "Let's remember Linear Regression:\n",
    "\n",
    "$Y = \\beta_0 + \\beta_1 X_1 + \\epsilon$\n",
    "\n",
    "What about non-linear behavior as below?\n",
    "\n",
    "<div>\n",
    "<img src=\"files/linear_regression_vs_non_linear_regression.png\" width=\"50%\" align='center'/>\n",
    "</div>\n",
    "\n",
    "If we add a new transformed feature $X^2_1$ we have a better fit (in red)\n",
    "\n",
    "$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X^2_1 + \\ldots + \\epsilon$\n",
    "\n",
    "We could engineer very complex features: $X^3, X^6, (X^2_1 * X^5_2), \\ldots$\n",
    "\n",
    "**An interesting property** : our $R^2$, on the train set, will keep increasing with every additional feature! Because it will have more \"tools\" to fit all the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1797089-8844-4908-a4e5-fa729c566c93",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Most of the times we're fighting against the overfitting phenomenon. To do this we can use various techniques such as **regularization**.\n",
    "\n",
    "Regularization means adding a penalty term to the Loss that increases with\n",
    "$\\beta$:\n",
    "\n",
    "$Regularized Loss = Loss(X, y, \\beta) + Penalty(\\beta)$\n",
    "\n",
    "Usually we're trying to minimize the loss. But we're adding a penalty which is an increasing function of $\\beta$, meaning that the greater are our coefficients, the stronger is the penalty.\n",
    "\n",
    "In other words, when we're trying to **minimize the loss**, there's a new constraint which is **not having too big coefficients**.\n",
    "\n",
    "- It forces the model to **shrink certains coefficients** or even **select less features**.\n",
    "- So it prevents **overfitting**.\n",
    "\n",
    "$\\hat{y} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X^2_1 + \\beta_3 X^3_1 + \\ldots$\n",
    "\n",
    "A model with smaller coefficients ($\\beta_1$, $\\beta_2$ etc.), or even equal to zeros, is a **more simple model**. Instead of taking all the features and trying to link them with all the other ones, our model will focus more on basic relationships, thus **avoiding modelizing the noise**.\n",
    "\n",
    "## Ridge (L2) and Lasso (L1)\n",
    "\n",
    "### Formulas\n",
    "\n",
    "The two most famous Regularization penalties are the Ridge and the Lasso:\n",
    "\n",
    "$Ridge(X, y, \\beta) = \\text{Loss}(X, y, \\beta) + \\alpha \\sum\\limits_{i=1}^{n} \\beta_i^2$\n",
    "\n",
    "With the ridge the penalty is the sum of the squares of the $\\beta$ coefficients.\n",
    "\n",
    "$Lasso(X, y, \\beta) = \\text{Loss}(X, y, \\beta) + \\alpha \\sum\\limits_{i=1}^{n} |\\beta_i|$\n",
    "\n",
    "With the lasso the penalty is the sum of the absolute values of the $\\beta$ coefficients.\n",
    "\n",
    "### New hyper-paramater : $\\alpha$\n",
    "\n",
    "- It dictates how much the model is regularized.\n",
    "- A large $\\alpha$ forces model complexity to decrease, so variance will be lowered and biais will be greater.\n",
    "\n",
    "### Ridge or Lasso?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b114a-69b3-4d89-99cc-ac8d042c0814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "X, y = load_diabetes(return_X_y=True, as_frame=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bff700d-eb57-441e-b2b3-5f852bc3ac39",
   "metadata": {},
   "source": [
    "### Documentation\n",
    "\n",
    "[On sklearn website](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset)\n",
    "\n",
    "- **age:** age in years\n",
    "- **sex:** male of female\n",
    "- **bmi:** body mass index\n",
    "- **bp:**  average blood pressure\n",
    "- **s1:** tc, total serum cholesterol\n",
    "- **s2:** ldl, low-density lipoproteins\n",
    "- **s3:** hdl, high-density lipoproteins\n",
    "- **s4:** tch, total cholesterol / HDL\n",
    "- **s5:** ltg, possibly log of serum triglycerides level\n",
    "- **s6:** glu, blood sugar level\n",
    "\n",
    "**Always scale your features before using regularization!** Here all values have already been scaled.\n",
    "\n",
    "The **target** variable is a quantitative measure of disease progression one year after baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897c518b-84af-4156-9230-55bd4a0b20a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb96aa9-9f10-416e-8b8b-16f0d572ea6d",
   "metadata": {},
   "source": [
    "In sklearn Ridge and Lasso are LinearRegression with a Ridge, or a Lasso, already implemented. You'll notice the $\\alpha$ coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa534f5-e0df-4469-803c-e53846855a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "\n",
    "# Creating 3 models\n",
    "linreg = LinearRegression().fit(X, y)\n",
    "ridge = Ridge(alpha=0.2).fit(X, y)\n",
    "lasso = Lasso(alpha=0.2).fit(X, y)\n",
    "\n",
    "coefs = pd.DataFrame({\n",
    "        \"coef_linreg\": pd.Series(linreg.coef_, index=X.columns),\n",
    "        \"coef_ridge\": pd.Series(ridge.coef_, index=X.columns),\n",
    "        \"coef_lasso\": pd.Series(lasso.coef_, index= X.columns)})\n",
    "\n",
    "coefs.map(lambda x: int(x)).style.map(lambda x: 'color: red' if x == 0 else 'color: black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68000f50-ffa8-474c-9216-ff31cced2ca3",
   "metadata": {},
   "source": [
    "The Linear Regression classic says that the disease progression is $-10 * age + (-239 * sex)$ etc.\n",
    "Some $\\beta$ are very large.\n",
    "\n",
    "- The ridge lowers all of our coef.\n",
    "- The lasso also lowers our coef and set to zeros some of them, even though they were the highest coefficient.\n",
    "\n",
    "Increasing the alpha parameter increasing the action of ridge and lasso.\n",
    "\n",
    "The features which are penalized are most of the time the ones which have the more noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316f0fbe-5a30-4827-a040-a378f8572c01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True, as_frame=True)\n",
    "\n",
    "def get_coef(model, alpha, X=X, y=y): return model(alpha=alpha).fit(X, y).coef_.round(2)\n",
    "\n",
    "def get_df(model, alpha_list, X=X, y=y):\n",
    "    _df = pd.DataFrame()\n",
    "    for alpha in alpha_list:\n",
    "        _df = pd.concat([_df, pd.DataFrame([np.append(get_coef(model, alpha=alpha), alpha)], columns=[col for col in X.columns] + ['alpha'])], axis=0)\n",
    "    return _df\n",
    "\n",
    "def plot_against_alpha(_df, alpha_name='alpha'):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    for column in _df.drop(columns=alpha_name):\n",
    "        ax.plot(_df[alpha_name], _df[column], label=column)\n",
    "    ax.set_xlabel('Alpha')\n",
    "    ax.set_ylabel('Coef')\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de31e3-3995-47f0-96f9-f60f2a7fd7ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alpha_list = np.linspace(0.1, 4, 20)\n",
    "ridge_df = get_df(Ridge, alpha_list)\n",
    "lasso_df = get_df(Lasso, alpha_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c422d1be-3f9b-4469-bef4-6b1fd0ca593a",
   "metadata": {},
   "source": [
    "#### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac569474-d2d4-41b4-9eae-d32ac314a6fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_against_alpha(ridge_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feab586f-14e2-42bc-ac22-422d68eafddd",
   "metadata": {},
   "source": [
    "#### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ebef3-7d51-42e5-8383-0d16e056a34b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_against_alpha(lasso_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2386d8-a661-4f02-978d-11eb2036625b",
   "metadata": {},
   "source": [
    "- Increasing $\\alpha$ in Ridge will only shrink parameters towards zero. As seen above.\n",
    "- Whereas increasing $\\alpha$ in Lasso can shrink parameter to 0 (natural feature selector). As you can see below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e212a9e7-878b-40cd-b4c1-13b9ecacc131",
   "metadata": {},
   "source": [
    "### An other way to see it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff3961b-1858-487b-bd03-1b713a12ffa6",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div>\n",
    "<img src=\"files/lasso_vs_ridge.png\" width=\"75%\" source='https://www.linkedin.com/pulse/ridge-lasso-regularization-gurumaheswara-reddy/' align='center'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2b90e2-c7b1-4568-9ee5-7320f30b868c",
   "metadata": {},
   "source": [
    "\"Regularization restricts the allowed positions of β̂ to the blue constraint region:\n",
    "\n",
    "- For **lasso**, this region is a diamond-shaped because it constrains the absolute value of the coefficients.\n",
    "\n",
    "- For **ridge**, this region is a circle-shaped because it constrains the square of the coefficients.\n",
    "\n",
    "The size of the blue region is determined by α, with a smaller α resulting in a larger region\n",
    "When α is zero, the blue region is infinitely large, and thus the coefficient sizes are not constrained.\n",
    "When α increases, the blue region gets smaller and smaller.\"\n",
    "\n",
    "[source](https://www.linkedin.com/pulse/ridge-lasso-regularization-gurumaheswara-reddy/)\n",
    "\n",
    "We're trying to minimize the loss function (red circles), from the optimal $\\hat\\beta$ but with the constraint that the sum of $\\beta^2_i$ (Ridge) or $|\\beta_i|$ (Lasso) is inferior to a constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517ae968-50aa-443c-9288-8938c104cde8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evolution of the Ridge Regularization whith different $\\alpha$ values\n",
    "<div>\n",
    "<img src=\"files/ridge_and_alpha.png\" width=\"75%\" source='https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/' align='center'/>\n",
    "</div>\n",
    "\n",
    "For Ridge, the importance of all coefficients is reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ca981-49a2-4b61-9453-78977401d122",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evolution of the Lasso Regularization whith different $\\alpha$ values\n",
    "<div>\n",
    "<img src=\"files/lasso_and_alpha.png\" width=\"75%\" source='https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/' align='center'/>\n",
    "</div>\n",
    "\n",
    "The higher $\\alpha$ is, the more features are being cut out. For alpha = 0.01, there's only one feature left, and from $\\alpha$ = 1, there's no features left, only the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e291ac59-7327-4443-8ff8-a5c4f386cd47",
   "metadata": {},
   "source": [
    "## ElasticNet = Lasso + Ridge\n",
    "\n",
    "ElasticNet = Lasso & Ridge Weighted Average\n",
    "\n",
    "$L = ||y - \\hat{y}||^2 + \\alpha (\\lambda |\\beta| + (1 - \\lambda)||\\beta||^2)$\n",
    "\n",
    "Now, we've got 2 hyper-parameters to fine-tune : $\\alpha$ and $\\lambda$.\n",
    "If $\\lambda$ = 1, it's going to output the same result than lasso. If it's close from zero, it's going to output the same result than Ridge. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de929d9-3fd0-49d6-9a62-5f10a1cf22d8",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"files/elasticnet.webp\" width=\"65%\" source='https://corporatefinanceinstitute.com/resources/data-science/elastic-net/' align='center'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbf20d0-bfca-4fff-b9fc-188a41db2a47",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "- Regularize when you think you are overfitting (learning curves not converging).\n",
    "- **Ridge** when you believe **all coefficients** may have an impact.\n",
    "- **Lasso** as a **feature selection tool** and for **better interpretability**.\n",
    "- Use **ElasticNet** if you need a bit of both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3bad92-7f91-4306-bd9a-18648055170b",
   "metadata": {},
   "source": [
    "## How do we find the best hyper-parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47bf1fb-4b5d-4bec-938c-fffebc9c8f21",
   "metadata": {},
   "source": [
    "### The Grid Search Method\n",
    "\n",
    "- Hold out a validation set (never use the test set for model tuning!)\n",
    "- Select which grid of values of hyper-parameters to try out\n",
    "- For each combination of values, measure your performance on the validation set\n",
    "- Select hyper-parameters that produce the best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d0d05f-b144-4b9e-98eb-6423c0c247a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state=1)\n",
    "\n",
    "# Select hyperparam values to try\n",
    "alphas = [0.01, 0.1, 1] # L1 + L2 \n",
    "l1_ratios = [0.2, 0.5, 0.8] # Lamba, otherwise : L1 / L2 ratio. So the higher it is the closer we have some L1 (Lasso)\n",
    "\n",
    "# create all combinations [(0.01, 0.2), (0.01, 0.5), (...)]\n",
    "from itertools import product\n",
    "hyperparams = product(alphas, l1_ratios) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68aa027-ba2c-42a5-9de4-deaa0d5db2fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train and CV-score model for each combination\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for hyperparam in hyperparams:\n",
    "    alpha = hyperparam[0]\n",
    "    l1_ratio = hyperparam[1]\n",
    "\n",
    "    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "\n",
    "    r2 = cross_val_score(model, X_train, y_train, cv=5).mean()\n",
    "\n",
    "    print(f\"alpha: {alpha}, l1_ratio: {l1_ratio},   r2: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cea80bc-411a-4823-81d6-46f0f7fa1448",
   "metadata": {},
   "source": [
    "There's a function already in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090aec12-98ad-46e1-a8f7-1f8c39d2e520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "\n",
    "# Instantiate model\n",
    "model = ElasticNet()\n",
    "\n",
    "# Hyperparameter Grid\n",
    "grid = {\n",
    "    'alpha': [0.01, 0.1, 1], \n",
    "    'l1_ratio': [0.2, 0.5, 0.8] # lambda (remember \"L1\" is also called \"Lasso\")\n",
    "}\n",
    "\n",
    "# Instantiate Grid Search\n",
    "search = GridSearchCV(\n",
    "    model,\n",
    "    grid, # dictionary of hyperparameter \n",
    "    scoring='r2', # or an other Metric for which we want to optimize the regulation\n",
    "    cv=5, # So validation set is 20% each time, then we get the mean.\n",
    "    n_jobs=-1, # parallelize computation\n",
    "    verbose=1,\n",
    ") \n",
    "\n",
    "# Fit data to Grid Search\n",
    "search.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ec881-a1e1-4904-b410-2fcd5bad2c69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Best score\n",
    "search.best_score_ # a numpy float64 object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63cb6f9-9de1-419b-9cfc-beaf63269c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Best Params\n",
    "search.best_params_ # dict object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c13da82-e89a-42a9-90d3-40ad7cde7a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Best estimator\n",
    "search.best_estimator_ # a sklearn object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d3b42c-8890-4b69-95b5-9d6ba4c081d6",
   "metadata": {},
   "source": [
    "### Limitations of Grid Search:\n",
    "\n",
    "- Computationally costly\n",
    "- The optimal hyperparameter value can be missed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f358e2cc-9072-4bdc-93a5-603ecbf0c9ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Random Search\n",
    "\n",
    "Instead of using a grid of parameter, we can aske sklearn to try random values. You can pass a list of random values or a distribution.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"files/grid_search_vs_random_search.webp\" width=\"55%\" source='https://medium.com/@cjl2fv/an-intro-to-hyper-parameter-optimization-using-grid-search-and-random-search-d73b9834ca0a' align='center'/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960a2c59-5397-4916-b222-117b87ed633c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy import stats\n",
    "\n",
    "# Instantiate model\n",
    "model = ElasticNet()\n",
    "\n",
    "# Hyperparameter Grid\n",
    "grid = {'l1_ratio': stats.uniform(0, 1),\n",
    "        'alpha': [0.001, 0.01, 0.1, 1]} # you can pass functions or list of values\n",
    "\n",
    "# Instantiate Grid Search\n",
    "search = RandomizedSearchCV(\n",
    "         model,\n",
    "         grid, \n",
    "         scoring='r2',\n",
    "         n_iter=100,  # number of cv trainings\n",
    "         cv=5, # number of folds for each cv\n",
    "         n_jobs=-1\n",
    "    ) \n",
    "\n",
    "# Fit data to Grid Search\n",
    "search.fit(X_train, y_train)\n",
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff83499-5d3c-4c1c-b7e3-d94ba3d2d030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate random numbers from a uniform distribution\n",
    "data = stats.uniform.rvs(0, 1, size=1000) # rvs = random variates\n",
    "\n",
    "plt.hist(data, bins=50, density=True, alpha=0.6, color='g')\n",
    "\n",
    "# Plot the probability density function (PDF)\n",
    "x = np.linspace(0, 1, 100)\n",
    "plt.plot(x, stats.uniform.pdf(x, 0, 1), 'r-', lw=2, label='Uniform PDF')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('Uniform Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b19dcb-5741-4032-95e0-331d607afcd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "# dist = stats.norm(10, 2) # if you have a best guess (say: 10)\n",
    "# dist = stats.randint(1,100) # if you have no idea\n",
    "# dist = stats.uniform(1, 100) # same\n",
    "# dist = stats.loguniform(0.01, 1) # Coarse grain search\n",
    "\n",
    "r = dist.rvs(size=10000) # Random draws\n",
    "plt.hist(r);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89549613-df07-4c19-8984-7d89eaa59c57",
   "metadata": {},
   "source": [
    "### RandomizedSearch vs GridSearch\n",
    "\n",
    "#### GridSearch\n",
    "\n",
    "- You want to explore a specific hyperparameter or combination.\n",
    "- You are optimizing few parameters.\n",
    "- You think all hyperparameters are equally important for performance.\n",
    "\n",
    "#### Randomized Search\n",
    "\n",
    "- Less typing, if you want to try many different values.\n",
    "- Control for the number of combinations to try and time spent searching.\n",
    "- Useful when some hyper-parameters are more important than others.\n",
    "\n",
    "#### Use Both\n",
    "\n",
    "You can actually **start with a random search** if you're not sure of what're you're looking for, then use a **grid search** to test all the combinations between the values you have found."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
