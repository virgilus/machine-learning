{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12f70f4e-89f4-47df-95cc-8b1120d577f7",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "**Unsupervised learning** is a type of machine learning where the algorithm learns patterns from input data without explicit supervision or labeled responses.\n",
    "\n",
    "In **unsupervised learning**, the algorithm tries to find hidden structures or relationships within the data.\n",
    "\n",
    "Unlike **supervised learning**, where the algorithm is provided with labeled data and learns to predict output based on input-output pairs, **unsupervised learning** deals with **unlabeled data** and aims to uncover underlying patterns or structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fad221-0035-4484-8ff7-88708279971e",
   "metadata": {},
   "source": [
    "## What if you only have X?\n",
    "\n",
    "You can use unsupervised learning to :\n",
    "\n",
    "- **Understand your data** (exploration, visualisation, segmentation...)\n",
    "- **Feature processing** (engineering, selection, compression...)\n",
    "- **Because you have no targets yet** (too tough to annotate, too expensive, huge dataset...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06636f7a-bb71-4d85-9b52-0fd51bf06afa",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "- Aims to find the best linear combination of features that best represents the underlying structure of the data.\n",
    "- Squashes our high-dimensional dataset down into a lower dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4dd1a-faec-4c72-8887-e833c5b28dd0",
   "metadata": {},
   "source": [
    "\n",
    "### Optimize linear combination of features\n",
    "\n",
    "Remember linear regression variants?\n",
    "\n",
    "**Polynomial**\n",
    "\n",
    "$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X^2$\n",
    "\n",
    "**Log transformation**\n",
    "\n",
    "$Y = \\beta_0 + \\beta_1\\log(X_1)$\n",
    "\n",
    "**Linear combination of features**\n",
    "\n",
    "$Y = \\beta_0 + \\beta_1X_1 + \\beta_2(X_2 + X_3)$\n",
    "\n",
    "With PCA you can find the best linear combination of features which:\n",
    "\n",
    "$Y = \\beta_0 + \\beta_1X'_1 + \\beta_2X'_2$\n",
    "\n",
    "- Removes any multicolinearity.\n",
    "- Ranks features by \"importance\". Meaning that the biggest part of the explainability will be in the first $X'_1$ than, $X'_2$ and so on...\n",
    "\n",
    "How do we build those $X'$, which are also called \"principal components\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d79d2e-4146-425c-9ad5-32d2740e297c",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"files/PCA.png\" width=\"75%\" align='center' source='https://www.biorender.com/template/principal-component-analysis-pca-transformation'/> </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f2c7c0-2e70-46f1-9f17-98324e2f307c",
   "metadata": {},
   "source": [
    "- PCA returns a new projection of the data.\n",
    "- On the image below, we start with three different features and we're looking for to create two new features PCA1 and PCA2 which are not colinear.\n",
    "\n",
    "- On the second graph, values vary a lot in function to PC1, but vary just a little with PC2.\n",
    "- The features are orthonomal to each other and maximize the variance explained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e83c87b-4adb-49ef-87d1-bbec80043813",
   "metadata": {},
   "source": [
    "[2D Interactive Visualization](https://setosa.io/ev/principal-component-analysis/)\n",
    "\n",
    "[Stats Stack Exchange](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1ae4db-24a1-4ad8-a458-fe491843046e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a4949-941a-4373-8411-e3d53d1418db",
   "metadata": {},
   "source": [
    "## Example with Wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f21cc8-d227-4257-9204-c2d99c8fd9d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine(as_frame=True)\n",
    "X = wine.data\n",
    "y = wine.target # we do have y, but let's forget about it for now\n",
    "wine_features = X.columns\n",
    "\n",
    "# Data must be centered around its mean before applying PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = pd.DataFrame(scaler.transform(X), columns=wine_features)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81feaf89-ea5d-4a0d-a3fc-2aa1dc3e10f5",
   "metadata": {},
   "source": [
    "### Heatmap and correlation\n",
    "\n",
    "Information of some features are also inside other features such as \"flavanoids\" and \"total_phenols\". There is redundancy inside the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddea851-4145-4483-87cc-8c71ce0c7c61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pd.DataFrame(X).corr(), cmap='coolwarm', vmin=-1, vmax=1, annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c41651c-264b-4642-99e6-136318ba4f89",
   "metadata": {},
   "source": [
    "In this case it's worth doing some PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf19685-53e7-4a5e-9ffe-154166e24824",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e6e8e2-c37e-432f-a618-45bcb8a5e006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# New 13 features as linear combination of initial vector basis (rows)\n",
    "#pd.DataFrame(pca.components_)\n",
    "# Access our 13 PCs \n",
    "W = pca.components_\n",
    "\n",
    "# Print PCs as COLUMNS\n",
    "W = pd.DataFrame(W.T,\n",
    "                 index=wine_features,\n",
    "                 columns=[f'PC{i}' for i in range(1, 14)])\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54f99e6-eafb-48dd-9da2-47d449f39100",
   "metadata": {},
   "source": [
    "So PC1 $= 0.144329 * X_1 \\text{(alcohol)} - 0.483652 * X_2 \\text{(malid_acid)} - 0.207383 * X_3 \\text{(ash)} \\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182f0a7e-ed3d-4e55-a7b7-d84f82de9aad",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Transforming X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a79a1a3-975b-4e4f-b3a1-18af64233b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_proj = pca.transform(X) # After fitting, we can transform our original X\n",
    "X_proj = pd.DataFrame(X_proj, columns=[f'PC{i}' for i in range(1, 14)]) # X_proj is now my new dataset with my 178 samples.\n",
    "X_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e5bdcf-934a-45c9-8c52-524a3e086e63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now Xp features are uncorrelated\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pd.DataFrame(X_proj).corr(), cmap='coolwarm', vmin=-1, vmax=1,);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd7a95-7997-4c83-8bba-efb627ea7294",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2D-slice\n",
    "\n",
    "plt.figure(figsize=(13,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('X1 vs. X0 before PCA (initial space)')\n",
    "plt.xlabel('X0')\n",
    "plt.ylabel('X1')\n",
    "plt.scatter(X.iloc[:,0], X.iloc[:,1])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('PC1 vs PC2 (new space)')\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.scatter(X_proj.iloc[:,0], X_proj.iloc[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca960e1-5df5-4aa2-9083-7cebd8727a2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Before the PCA, nothing is really important. After the PCA a \"space\" has been created, it looks like the data is more \"clusterised\". But it's only 2 features out of the 13 there are.\n",
    "\n",
    "### Share of variance explained by each PC\n",
    "\n",
    "Also now PC1 will contain most of the \"explainability\".\n",
    "\n",
    "$\\frac{Var(Pc)}{Var(X)}$\n",
    "\n",
    "How much the variation of only one PC explains the variation of my X?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9de9e8-1228-4c08-b99a-7dde58cc94d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_ # Share of varince explained by each of the selected components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37951b79-6563-4111-9a26-173941380d10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PC1 explains 36% of the variation, PC2 19%, PC3 11% etc.\n",
    "\n",
    "plt.plot(pca.explained_variance_ratio_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab698a3-608d-4292-a7fa-971bc32dd9fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can compute these variances\n",
    "X_proj.var() / (X_proj.var()).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a540a0a8-7dd0-48e0-8d10-1e0cef12aa75",
   "metadata": {},
   "source": [
    "### How do you compute PCA?\n",
    "\n",
    "1. Standardize the range of continuous initial variables\n",
    "2. Compute the covariance matrix to identify correlations\n",
    "3. Compute the eigenvectors and eigenvalues of the covariance matrix to identify the principal components\n",
    "4. Create a feature vector to decide which principal components to keep\n",
    "5. Recast the data along the principal components axes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a68c06-8519-45af-ab3e-8e7654d2cae2",
   "metadata": {},
   "source": [
    "### Why would you want less features?\n",
    "\n",
    "- To compress data\n",
    "- To reduce model complexity\n",
    "- To reduce overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80159394-c8ad-48b1-8db3-afa76f8b6298",
   "metadata": {
    "tags": []
   },
   "source": [
    "### How many features to keep ($k$)?\n",
    "\n",
    "If we take a look at the graph above, we see there's an inflection point starting from PCA3. Choosing $k$ is a trade-off between compression and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7869d98c-b85f-4481-8f5e-e88c66338217",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.ylim(ymin=0)\n",
    "plt.title('cumulated share of explained variance')\n",
    "plt.xlabel('# of principal component used');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdd73e2-1590-416e-a4bc-63eb6ca3cd13",
   "metadata": {},
   "source": [
    "### PCA with fewer components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422d0563-153b-43eb-878c-90c5706b6854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit a PCA with only 3 components\n",
    "pca3 = PCA(n_components=3).fit(X)\n",
    "\n",
    "# Project your data into 3 dimensions\n",
    "X_proj3 = pd.DataFrame(pca3.fit_transform(X), columns=['PC1', 'PC2', 'PC3'])\n",
    "\n",
    "# We have \"compressed\" our dataset in 3D\n",
    "X_proj3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9780133f-dab4-49f7-b658-c71edf0e5ac3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(\"accuracy all 13 initial features\")\n",
    "print(cross_val_score(LogisticRegression(), X, y, cv=5).mean())\n",
    "\n",
    "print(\"\\n accuracy 3 PCs\")\n",
    "print(cross_val_score(LogisticRegression(), X_proj3, y, cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069075fc-8a7f-448e-aa7d-a98565c929dc",
   "metadata": {},
   "source": [
    "### Decompress\n",
    "\n",
    "Can you reconstruct exactly X from X_proj?\n",
    "\n",
    "Not if you kept k < 13 dimensions; information has been lost\n",
    "We can approximate X by reconstructing it with inverse_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf27e8c4-6bbc-4792-84e0-b7089acc7bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_reconstructed = pca3.inverse_transform(X_proj3)\n",
    "X_reconstructed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685786bf-5fdc-4140-813b-28c7a0c4efa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(1,2,1)\n",
    "sns.heatmap(X)\n",
    "plt.title(\"original data\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"reconstructed data\")\n",
    "sns.heatmap(X_reconstructed);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae2d381-38e2-4b4a-a99a-c301dc203b37",
   "metadata": {},
   "source": [
    "### Limitations of PCA\n",
    "\n",
    "- Cannot capture non linear data. But there is kernel PCA to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4c0068-ace4-41ee-b978-edee8ccb90ef",
   "metadata": {},
   "source": [
    "## Clustering (with K-Means)\n",
    "\n",
    "### Definition\n",
    "\n",
    "The process of organizing data points into groups whose members are similar in some way. So we're going to find categories (classes, segments) of unlabelled data rather than just trying to reduce dimensionality.\n",
    "\n",
    "<div>\n",
    "<img src=\"files/clustering.webp\" width=\"75%\" align='center' source='https://bolisettigunasekhar.medium.com/what-is-clustering-7c8c9c34bd66'> </div>\n",
    "\n",
    "[Animation](https://shabal.in/visuals/kmeans/2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5b2295-6960-4c71-8694-118e078e6b0b",
   "metadata": {},
   "source": [
    "### K-means algorithm\n",
    "\n",
    "The K-means algorithm is actually quite simple.\n",
    "\n",
    "1. Select the number $K$ (hyperparameter) to decide the number of clusters.\n",
    "\n",
    "1. Select random $K$ points or centroids. (It can be other from the input dataset).\n",
    "\n",
    "1. Assign each data point to their closest centroid, which will form the predefined $K$ clusters.\n",
    "\n",
    "1. Calculate the variance and place a new centroid of each cluster.\n",
    "\n",
    "1. Repeat the third steps, which means reassign each datapoint to the new closest centroid of each cluster.\n",
    "\n",
    "1. If any reassignment occurs, then go to step-4 else go to FINISH.\n",
    "\n",
    "1. The model is ready.\n",
    "\n",
    "[source](https://www.javatpoint.com/k-means-clustering-algorithm-in-machine-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd245f9-0190-48a8-a082-f3b0dcc1b997",
   "metadata": {
    "tags": []
   },
   "source": [
    "### K-means and dimensions\n",
    "\n",
    "If you plot two features, maybe you won't see there are different clusters. But if you add a feature, it might become obvious.\n",
    "\n",
    "<div>\n",
    "<img src=\"files/k-means_2D_to_3D.png\" width=\"65%\" align='center' source='https://bolisettigunasekhar.medium.com/what-is-clustering-7c8c9c34bd66'> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ff02cb-4f80-4c6e-a286-5e5dd0f9da79",
   "metadata": {},
   "source": [
    "So in order to the algorithm work, it's better if the data is already \"clustered\" geomatrically. You can use PCA first to better shape your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110f15fa-aed5-43e7-a6f5-189ef7cf2f9a",
   "metadata": {},
   "source": [
    "### In practice\n",
    "\n",
    "- K-means is usually run a few times with different random initializations (sklearn will do it by itself).\n",
    "- We can use a random mini-batch at each epoch instead of the full dataset.\n",
    "- The algorithm is quite fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68905112-3182-4320-ad55-b57a73c469db",
   "metadata": {},
   "source": [
    "### Wine clustering\n",
    "\n",
    "Let's go back to our wine dataset on which we applied a PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d7b76-7031-4cc3-83c4-8cf1a29a7c66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Fit K-means\n",
    "km = KMeans(n_clusters=3)\n",
    "km.fit(X_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ebb89a-80de-4c23-9a86-b910d018f993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "km.cluster_centers_.shape # Position in 13 dimensions of the 3 centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c0f20e-e9c1-45ea-a93d-f18cb3fc1faf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "km.labels_ # All our samples (data points) are already labelled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5354a531-e5c2-4b87-b222-eafee18ba0c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the first two PC (which contain the more information as we've seen)\n",
    "plt.scatter(X_proj.iloc[:,0], X_proj.iloc[:,1], c=km.labels_, cmap='viridis_r')\n",
    "plt.title('KMeans clustering')\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfb07db-458d-40d0-af74-cace5bfadb91",
   "metadata": {},
   "source": [
    "### Mapping ```y``` and labels\n",
    "\n",
    "Watch out! The km.labels_ and the y may not match together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b9efe-98c9-485b-880b-d54c52233685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set(km.labels_)\n",
    "np.unique(km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b11439-517a-4a39-a8d4-a455ee5ef968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This seems right... But !\n",
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd5a648-a317-487f-afaa-0eaf40b40d68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.unique(km.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bd4d58-160f-4ff8-bf16-b911d639a7dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#y.value_counts()\n",
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6df7d86-dc87-45fc-baab-ab393a3f9af9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here are a few options of how you can do the matching.\n",
    "\n",
    "# With list comprehension\n",
    "km_labels_mapped = np.array([0 if x == 2 else 1 if x == 0 else 2 for x in km.labels_])\n",
    "# With numpy vectorize\n",
    "km_labels_mapped = np.vectorize(lambda x: {0:1, 1:2, 2:0}.get(x, 0))(km.labels_)\n",
    "# With Pandas map\n",
    "km_labels_mapped = np.array(pd.Series(km.labels_).map({0:1, 1:2, 2:0}))\n",
    "km_labels_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9015ec5c-5ffd-42d5-b254-3874a96f40f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualization of y vs y_pred\n",
    "plt.figure(figsize=(13,5))\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X_proj.iloc[:,0], X_proj.iloc[:,1], c=y, cmap='viridis')\n",
    "plt.title('True wine labels'); plt.xlabel('PC 1'); plt.ylabel('PC 2');\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X_proj.iloc[:,0], X_proj.iloc[:,1], c=km_labels_mapped, cmap='viridis')\n",
    "plt.title('KMeans clustering'); plt.xlabel('PC 1'); plt.ylabel('PC 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f699f2-155d-4108-bdcd-f7bc512d6c9d",
   "metadata": {},
   "source": [
    "### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5ecc48-1152-4c9c-801c-e8bb170cab87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = pd.Series(km_labels_mapped)\n",
    "#y_pred = pd.Series(km.labels_).map({0:1, 1:2, 2:0}) # Alternative way, don't forget to change the mapping manually!\n",
    "accuracy_score(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccecd2ea-77d1-4cf7-8062-295344f9ebf7",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "We can then use the fitted model to predict new values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9cb162-a8ab-4f1f-beec-7d40ab22f8cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating a new df with the right column names a random observation\n",
    "new_obs = pd.DataFrame(data=np.random.random((1,13)), columns=X_proj.columns) # np.random.random((1,13)) -> 1 row and 13 columns\n",
    "km.predict(new_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9178e4e2-8ac3-40b8-b70a-45bf1990a123",
   "metadata": {
    "tags": []
   },
   "source": [
    "### K-Means' Loss Function\n",
    "\n",
    "```km.fit(X)``` finds parameters $\\beta$ that minimize a loss.\n",
    "\n",
    "Each $\\beta_j$ parameter is the centroid $\\mu_j$ of its respective cluster $C_j$.\n",
    "\n",
    "The loss function is called inertia $L(\\mu)$\n",
    "\n",
    "= **sum of squared distance** between each observation and their closest centroid\n",
    "\n",
    "= sum of **within-cluster sum of squares** (WCSS)\n",
    "\n",
    "= variance\n",
    "\n",
    "$inertia = L(\\mu) = K\\sum_{j=1}^{\\text{K}} \\sum_{x_i \\in C_j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b9e489-e98e-4271-8506-e380025444b9",
   "metadata": {},
   "source": [
    "### Choosing $K$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4255f2-79ca-40ce-9933-41b8a069ea39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inertias = []\n",
    "ks = range(1,10)\n",
    "\n",
    "for k in ks:\n",
    "    km_test = KMeans(n_clusters=k).fit(X)\n",
    "    inertias.append(km_test.inertia_)\n",
    "\n",
    "plt.plot(ks, inertias)\n",
    "plt.xlabel('k cluster number')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b2e1ba-212a-4d60-955a-7aa518feaf3c",
   "metadata": {},
   "source": [
    "### There are many other types of clustering\n",
    "\n",
    "<div>\n",
    "<img src=\"files/sklearn_clustering.png\" width=\"75%\" align='center' source='https://bolisettigunasekhar.medium.com/what-is-clustering-7c8c9c34bd66'> </div>\n",
    "\n",
    "[Sklearn website](https://scikit-learn.org/stable/modules/clustering.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
