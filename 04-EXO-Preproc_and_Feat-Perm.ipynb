{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise will guide you through the preprocessing workflow. Step by step, feature by feature, you will investigate the dataset and take preprocessing decisions accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "selected_features = ['GrLivArea',\n",
    "                     'BedroomAbvGr',\n",
    "                     'KitchenAbvGr', \n",
    "                     'OverallCond',\n",
    "                     'RoofSurface',\n",
    "                     'GarageFinish',\n",
    "                     'CentralAir',\n",
    "                     'ChimneyStyle',\n",
    "                     'MoSold',\n",
    "                     'SalePrice']\n",
    "\n",
    "df = pd.read_csv('data/iowa_housing.csv', usecols=selected_features)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, you have a file named \"iowa_housing.txt\" in the \"data\" folder with informations about the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Duplicates in datasets cause data leakage.*** \n",
    "\n",
    "It is important to locate and remove duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **>>>** How many duplicated rows are there in the dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **>>>** Remove the duplicates from the dataset. Overwite the dataframe `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **>>>** Print the percentage of missing values for every column of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GarageFinish`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **>>>** Investigate the missing values in `GarageFinish`. Then, choose one of the following solutions:\n",
    "\n",
    "1. Drop the column entirely.\n",
    "2. Impute the column median using `SimpleImputer` from Scikit-Learn.\n",
    "3. Preserve the NaNs and replace them with meaningful values.\n",
    "\n",
    "Modify the Series `GarageFinish` accordingly. (Do not replace the entire DataFrame with your new Series!).\n",
    "\n",
    "**HINT**: According to the dataset description, the missing values in `GarageFinish` represent a house having no garage. They need to be encoded as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `RoofSurface`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **>>>** Investigate the missing values in `RoofSurface`. Then, choose one of the following solutions:\n",
    "\n",
    "1. Drop the column entirely\n",
    "2. Impute the column median using sklearn's `SimpleImputer`\n",
    "3. Preserve the NaNs and replace them with meaningful values\n",
    "\n",
    "**HINT**: `RoofSurface` has a few missing values that can be imputed by the median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ChimneyStyle`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **>>>** Investigate the missing values in `ChimneyStyle`. Then, choose one of the following solutions:\n",
    "\n",
    "1. Drop the column entirely\n",
    "2. Impute the column median\n",
    "3. Preserve the NaNs and replace them with meaningful values\n",
    "\n",
    "**HINT** Be careful: not all missing values are represented as `np.nans`, and Python's `isnull()` only detects `np.nans`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **>>>** When you are done with handling missing value, print out the percentage of missing values for the entire dataframe.\n",
    "\n",
    "You should no longer have missing values !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First of all, before scaling...**\n",
    "\n",
    "To understand the effects of scaling and encoding on model performance, let's use a linear regression and get a **base score without any data transformation**.\n",
    "\n",
    "❓ **>>>** Cross-validate a linear regression model that predicts `SalePrice` using the other features.\n",
    "\n",
    "⚠️ Note that a linear regression model can only handle numeric features. [DataFrame.select_dtypes](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.select_dtypes.html) can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **Expected results** : About ```0.577```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `RoofSurface` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **>>>** **Question** about `RoofSurface`\n",
    "\n",
    "Investigate `RoofSurface` for distribution and outliers, you can use a ```.boxplot()``` and / or a ```.plot.hist()```. Then, choose the most appropriate scaling technique. Either:\n",
    "\n",
    "1. Standard Scaler\n",
    "2. Robust Scaler\n",
    "3. MinMax Scaler\n",
    "\n",
    "Replace the original columns with the transformed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GrLivArea`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **>>>** Investigate `GrLivArea` for distribution and outliers. Then, choose the most appropriate scaling technique. Either:\n",
    "\n",
    "1. Standard Scaler\n",
    "2. Robust Scaler\n",
    "3. MinMax Scaler\n",
    "\n",
    "Replace the original columns with the transformed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BedroomAbvGr` ,  `OverallCond` & `KitchenAbvGr`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **>>>** Investigate `BedroomAbvGr`, `OverallCond` & `KitchenAbvGr`. Then, chose one of the following scaling techniques:\n",
    "\n",
    "1. MinMax Scaler\n",
    "2. Standard Scaler\n",
    "3. Robust Scaler\n",
    "\n",
    "Replace the original columns with the transformed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GarageFinish`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **>>>** Investigate `GarageFinish` and choose one of the following encoding techniques accordingly:\n",
    "- Ordinal encoding\n",
    "- One-Hot encoding\n",
    "\n",
    "Add the encoding to the dataframe as new colum(s), and remove the original column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding  `CentralAir`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **>>>** Investigate `CentralAir` and choose one of the following encoding techniques accordingly:\n",
    "- Ordinal encoding\n",
    "- One-Hot encoding\n",
    "\n",
    "Replace the original column with the newly generated encoded columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `MoSold` - Cyclical engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A feature can be numerical (continuous or discrete), categorical or ordinal. But a feature can also be temporal (e.g. quarters, months, days, minutes, ...). \n",
    "\n",
    "Cyclical features like time need some specific preprocessing. Indeed, if you want any Machine Learning algorithm to capture this cyclicity, your cyclical features must be preprocessed in a certain way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the feature `MoSold`, the month on which the house was sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"MoSold\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Many houses were sold in June (6), July (7) and May (5) (Spring/Summer)\n",
    "* Only a few houses were sold in December (12), January (1) and February (2) (~ Fall/Winter)\n",
    "    * But for any Machine Learning model, there is no reason why December (12) and January (1) would be \"close\"..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***How to deal with cyclical features?***\n",
    "\n",
    "The sort answer is : using $\\cos$ and $\\sin$!\n",
    "\n",
    "You can read more about this in this [article](https://www.sefidian.com/2021/03/26/handling-cyclical-features-such-as-hours-in-a-day-for-machine-learning-pipelines-with-python-example/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MoSold**\n",
    "- Let's create two new features `sin_MoSold` and `cos_MoSold` which correspond respectively to the sine and cosine of MoSold.\n",
    "- Drop the original column `MoSold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "months_in_a_year = 12\n",
    "\n",
    "df['sin_MoSold'] = np.sin(2*np.pi*(df['MoSold']) / months_in_a_year)\n",
    "df['cos_MoSold'] = np.cos(2*np.pi*(df['MoSold']) / months_in_a_year)\n",
    "df = df.drop(columns='MoSold')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the preprocessed dataset (optional)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "df.to_csv(\"data/my_iowa_preprocessed.csv\", index=False) # your file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's all start with the same dataset!\n",
    "df = pd.read_csv(\"data/iowa_preprocessed.csv\") # already created file\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **>>>** Plot a heatmap of the ***Pearson Correlation*** between the columns of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **>>>** Visualize the correlation between column pairs in a dataframe. You can use ```.stack()``` and then filter them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **>>>** How many pairs of features exceed a correlation of 0.9 or -0.9 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **>>>** Prepare the feature set `X` and target `y`.\n",
    "\n",
    "*Remember that we want to model the `SalePrice` with the preprocessed features.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **>>>** Cross validate a Linear Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "✅ **Expected results** : About ```0.645``` for a ```cv=10```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Permutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **>>>** Perform a feature permutation  and rank the features by order of importance. To display the coefficients you can use functions such as ```np.vstack``` or ```np.ndarray.reshape(1, -1)``` or ```.T``` (transposed), to better visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **>>>** Which feature is the most important ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write your answer (or code here) !\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling with less complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **>>>** Drop the weak features and cross-validate a new model.\n",
    "You should aim at maintaining a score close to the previous one. \n",
    "\n",
    "**Hint** : You can try dropping features one by one starting from the ones lowest importance until your model score starts dropping significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **Expected results** : About ```0.637``` for a ```cv=10```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
