{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will model the performance of an NBA player's win rating (`win_rating`) according to their game statistics (like minutes played, `mp`).\n",
    "\n",
    "The columns are:\n",
    "- `season`: The season (yearly) the player played in.\n",
    "- `poss`: Possessions played.\n",
    "- `mp`: Minutes played.\n",
    "- `do_ratio`: A player's ratio of time spent in defense vs. offense; negative values mean more defense positioning.\n",
    "- `pacing`: Player impact on team possessions per 48 minutes.\n",
    "- `win_rating`: *Wins Above Replacement* rating, how many additional wins a player is worth over a same-level replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **>>>** Load the `nba.csv` dataset located in the 'data' folder into this notebook as a pandas dataframe, display its first 5 rows and the shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Feature Set and Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first objective is to model the players' overall performance rating compared to peers, called *Wins Above Replacement*, (`win_rating`) against the minutes that they've played (`mp`).\n",
    "\n",
    "‚ùì **>>>** Assign those two variables to `X` and `y`. Remember that `X` is the feature(s), and `y` is the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **>>>** In a [scatter plot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html), visualize the relationship between the rating and the minutes played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot should hint that there is a relationship. But is it a perfectly linear relationship? We'll see!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **>>>** Using Sklearn's [`cross_validate`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html), run a 5-fold cross-validation on a [`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) model predicting the player performance rating from minutes played. Save the raw output of the cross-validation under a new variable called `cv_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **>>>** What is the lowest score of the cross-validation? Compute your answer and save the value under a new variable called `min_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **>>>** What is the highest score of the cross-validation?  Compute your answer and save the value under a new variable called `max_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **>>>** What is the mean score of the cross-validation? Compute your answer and save the value under a new variable called `mean_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ÑπÔ∏è When running a cross-validation, we always look at the mean score as the most robust and representative evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Expected results** : The mean here for a cross validation with $k$ = 5, should be between 0.5 and 0.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation does not train a model, it evaluates a hypothetical model on the dataset. If you want to use the model to, for example, make predictions, you will need to train it outside of the cross-validation.\n",
    "\n",
    "‚ùì **>>>** Go ahead and train the model on the full `X` and `y` (as we've already validated the model's score, and now will use it to predict). Save the trained model under the variable `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **>>>** What is the slope of your trained model? It can be accessed via the model's attributes. Save the slope under a variable named `slope`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **>>>** What is the intercept of your trained model? It can be accessed via the model's attributes. Save the intercept under a variable named `intercept`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Intercept\n",
    "intercept = model.intercept_\n",
    "intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **>>>** Use your trained model to predict the performance rating of a player who has played 1000 minutes in total. Save the predicted value in a variable called `prediction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Expected results** : The prediction should be around ```1.056```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curves are used to diagnose the performance of the model in more depth.\n",
    " \n",
    "‚ùì **>>>** Plot the learning curves of the model ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html)). \n",
    "\n",
    "For the training sizes, you can start with **100** rows and increment by **100** until **80%** of the dataset is used in training (**3200**). Hence, you should end up with **32** slices. [np.arange](https://numpy.org/doc/stable/reference/generated/numpy.arange.html) can help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "# Code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **>>>** How would you interpret the learning curves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write your reponse as a comment here:\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> ‚ÑπÔ∏è Unfold this cell to see our interpretation </summary>   \n",
    "    \n",
    "<br/>\n",
    "    \n",
    "We are observing **underfitting**\n",
    "\n",
    "The curves should have converged at a low score (be conscious of the scale: sometimes they look far apart, but their score is very close! You can try changing <code>plt.ylim()</code> to have a clearer view).\n",
    "- Training score has gone down substantially training size increased\n",
    "- Testing score has barely gone up - less than 1%! - even with 80% of the dataset being taken for training.\n",
    "\n",
    "There are two typical reasons that cause underfitting:\n",
    "- The model is **too simple** to learn the patterns of the data\n",
    "- The model needs **more features** to get better at predicting player win rating\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, as we saw the model performance doesn't seem optimal. We can try fixing it by adding more features.\n",
    "\n",
    "‚ùì **>>>** Cross validate a model made to predict player win rating with:\n",
    "- Minutes played (`mp`)\n",
    "- Possessions (`poss`)\n",
    "- Defense/offense ratio (`do_ratio`)\n",
    "- Pacing (`pacing`)\n",
    "\n",
    "Save the new cross-validated score under a variable named `score_added_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Expected results** : The performance prediction should be around ```0.632``` meaning the model is now better!\n",
    "\n",
    "Adding features provides the model with additional information from which to learn and with which to model the pattern of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **>>>** Plot the learning curves of the new model to evaluate its performance further (you can play with [plt.ylim()](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.ylim.html) to make the curves more obvious). You can reuse the previous code, or create a new one, or define a custom function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **>>>** How would you interpret the learning curves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write your answer\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> ‚ÑπÔ∏è Unfold this cell to see our interpretation </summary>   \n",
    "    \n",
    "<br/>\n",
    "    \n",
    "There's an improvement, but there still appears to be some **underfitting**.\n",
    "\n",
    "The curves should have converged, but the training score still drops substantially more than the testing score increases.\n",
    "\n",
    "Adding more features helped a little, but didn't fully solve the underfitting problem. So we can look into the other typical case:\n",
    "- The model is **too simple** to learn the patterns of the data\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Linear Model Fit with Polynomial Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've done the best we could with the features from our dataset, but we're still seeing some signs of underfitting.\n",
    "\n",
    "It might be that relying on linear combinations of features makes our model **too simple** for the relationship between win rating and the features of players - we can try to fix that with some **feature engineering**.\n",
    "\n",
    "Most of our ability to explain/predict player win rating has come from the minutes played (`mp`) feature. Let's look into this feature more.\n",
    "\n",
    "Plot a scatterplot of the relationship between these two columns. Feel free to use seaborn or matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='mp', y='win_rating', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a `LinearRegression` model with `mp` and `win_rating` with a **holdout**.\n",
    "\n",
    "Let's plot the learned regression line on the same plot to see how well it fits the data. Remember, you can extract the coefficients of a trained linear model with `coef_` and `intercept_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training the model\n",
    "model = LinearRegression()\n",
    "\n",
    "X = df[['mp']]\n",
    "y = df['win_rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# scoring the model\n",
    "lin_reg_score = model.score(X_test, y_test)\n",
    "print(\"Model R2:\", lin_reg_score)\n",
    "\n",
    "# extracting the coefficients and regression function\n",
    "regression = model.coef_[0] * df['mp'] + model.intercept_\n",
    "\n",
    "# plotting the data and learned regression function\n",
    "sns.scatterplot(data=df, x='mp', y='win_rating', alpha=0.5)\n",
    "plt.plot(df['mp'], regression, color='red', linewidth=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **>>>** Do you see what could be causing our model to underfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here!\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary> ‚ÑπÔ∏è Unfold this cell to see our interpretation </summary>   \n",
    "    \n",
    "<br/>\n",
    "    \n",
    "It looks like we're trying to fit a straight line learned by the Linear Regression model on data that is distributed *curvilinearly*.\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try improving the model by adding [**Polynomial Features**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures).\n",
    "\n",
    "Polynomial features are products of existing features of a set degree. For example, if we have two features - `a` and `b` - and we add degree-2 polynomial features, we'd end up with a feature set of [`a`, `b`, `a`$^2$, `a` $*$ `b`, `b`$^2$].\n",
    "\n",
    "Thanks to the exponents created by sklearn's `PolynomialFeatures`, we can train a model to closer represent curvilinear relationships, like the one observed between players' win ratings and minutes played.\n",
    "\n",
    "Let's try it! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update our feature set `X` with degree-2 polynomial features. Check the [example in the sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#Examples:~:text=linear_model/plot_polynomial_interpolation.py-,Examples,-%3E%3E%3E) to see how to transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "polynomial_features = PolynomialFeatures(degree=2, include_bias=False) # we don't want to add a column of 1's\n",
    "X_poly = polynomial_features.fit_transform(X)\n",
    "\n",
    "X_poly = pd.DataFrame(X_poly) # turning it back into a DataFrame for easier manipulation\n",
    "X_poly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that column names disappeared due to the transformation - feel free to update them, or just keep in mind that the first column is our original `mp` and the new column - `mp`$^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if adding a `degree=2` polynomial feature helps the model better represent the relationship between minutes played and a player's win rating.\n",
    "\n",
    "Run the cell below to see the new regression line, created by a model trained on degree-2 polynomial features, meaning `mp` and `mp`$^2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Here, we sort out data so that when we plot our final \n",
    "## line on our scatterpot it looks smooth.\n",
    "## You won't need to do this later on in the exercise!\n",
    "\n",
    "sorted_df = df.sort_values('mp')\n",
    "\n",
    "X_sorted = sorted_df[['mp']]\n",
    "y_sorted = sorted_df['win_rating']\n",
    "\n",
    "#creating our polynomial features\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X_sorted)\n",
    "\n",
    "#fit model\n",
    "model.fit(X_poly, y_sorted)\n",
    "\n",
    "predictions = model.predict(X_poly)\n",
    "\n",
    "#plot predictions over original data\n",
    "sns.scatterplot(x=X_sorted['mp'], y=y_sorted, alpha=0.5)\n",
    "plt.plot(X_sorted['mp'], predictions, linewidth=3, color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° The new regression line seems to be a better fit for our data - we're on the right track!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking the best number of degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to our **full feature set** - `['mp', 'poss', 'do_ratio', 'pacing']`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen that Polynomial Features on `mp` improves the model, but how about the rest of the dataset?\n",
    "\n",
    "‚ùì **>>>** Cross-validate a model trained on `degree=2` polynomial features of the whole dataset. How does the score change from previous models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Expected results** : The mean of the test score from the cross validation should be around ```0.869``` meaning the model is even better!\n",
    "\n",
    "The model score should have improved substantially! But how do we know if `degree=2` is the best one, and not 3, or 5, or 10...? Giving our model more degrees of freedom should only make the model better, right? ü§î\n",
    "\n",
    "‚ùì **>>>** Do cross-validation with `degree` of polynomial features ranging from **1** to **10**. Save all the scores, then plot them with their respective degrees to pinpoint the best result. You can use a ```for``` loop to do this.\n",
    "\n",
    "**NOTE:** It might take a while to run the full loop, as higher number of polynomial degrees creates exponentially more features. You can use the magic expression ```%%time``` at the start of the cell to check how long it took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X = df[['mp', 'poss', 'do_ratio', 'pacing']]\n",
    "y = df['win_rating']\n",
    "\n",
    "degrees = list(range(1,11))\n",
    "\n",
    "scores = []\n",
    "\n",
    "# Code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plotting the scores against degrees\n",
    "\n",
    "\n",
    "# To make sure the notebook doesn't return an error,\n",
    "# this cell is in \"raw\" mode : press esc + Y to make it code again!\n",
    "\n",
    "\n",
    "\n",
    "# Make sure the variable \"degrees\" contains a list of all the degrees you're testing\n",
    "# And the variable \"scores\" contains a list of all scores associated with the degrees.\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.plot(degrees, scores, label='CV Score (r2)', marker='o')\n",
    "\n",
    "# annotating each marker with the score\n",
    "for index, degree in enumerate(degrees):\n",
    "    plt.annotate(round(scores[index], 3), (degree-0.25, scores[index]+0.01))\n",
    "    \n",
    "# fixing the ticks so they are not displayed as floats\n",
    "plt.xticks(degrees[::2])\n",
    "\n",
    "# adding titles and labels to the plot\n",
    "plt.ylabel('CV Score (R2)', fontsize=14)\n",
    "plt.xlabel('# of Polynomial Degrees', fontsize= 4)\n",
    "plt.title('Evolution of CV Scores and Polynomial Feature Degrees', fontsize=18, y=1.03)\n",
    "\n",
    "plt.ylim(0,1)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **>>>** Which polynomial feature degree is best suited to predict NBA players' win rating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write your answer here\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "<details>\n",
    "<summary> ‚ÑπÔ∏è Click here to validate your answer </summary>   \n",
    "    \n",
    "You should be able to see that `degree=2` does give us the best score - now up to **0.87** after adding back the rest of the features!\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what happens after `degree=5` - with *even more* information the model results start going down??\n",
    "\n",
    "Let's plot the learning curves of a model with `degree=5` polynomial features to try and pinpoint the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform our X to include polynomial features\n",
    "poly_features = PolynomialFeatures(degree=5, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "# Get train scores, train sizes, and validation scores using `learning_curve`, r2 score\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    estimator = LinearRegression(),\n",
    "    X=X_poly, \n",
    "    y=y, \n",
    "    train_sizes=train_sizes, \n",
    "    cv=5)\n",
    "\n",
    "# Take the mean of cross-validated train scores and test scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "# Plot the learning curves!\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_sizes, train_scores_mean, label = 'Training score')\n",
    "plt.plot(train_sizes, test_scores_mean, label = 'Test score')\n",
    "plt.ylabel('r2 score', fontsize = 14)\n",
    "plt.xlabel('Training set size', fontsize = 14)\n",
    "plt.title('Learning curves', fontsize = 18, y = 1.03)\n",
    "plt.ylim(0,1)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **>>>** How would you interpret these learning curves?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> ‚ÑπÔ∏è Unfold this cell to see our interpretation </summary>   \n",
    "    \n",
    "<br/>\n",
    "    \n",
    "We see obvious **overfitting**!\n",
    "    \n",
    "üëâ The curves don't converge, the model is overfitting the training data with a training score (`~0.9`) that stays substantially higher than the testing score (`~0.7`).\n",
    "    \n",
    "üëâ By adding degree-5 polynomial features, there appears too much \"noise\" that the model pays attention to and the learned coefficients do not represent reality any more.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Training Set Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've found the best model score we could get, let's see if we can afford to train the model on less of data to save computation resources.\n",
    "\n",
    "Let's plot the learning curves of our best model - all features with `degree=2` polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the training size slices\n",
    "train_sizes = np.linspace(100, 3200, 32, dtype='int')\n",
    "\n",
    "X_poly = PolynomialFeatures(degree=2, include_bias=False).fit_transform(X)\n",
    "\n",
    "# Get train scores, train sizes, and validation scores using `learning_curve`, r2 score\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    estimator = LinearRegression(),\n",
    "    X = X_poly, \n",
    "    y = y, \n",
    "    train_sizes = train_sizes, \n",
    "    cv = 5\n",
    ")\n",
    "\n",
    "# Take the mean of cross-validated train scores and validation scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "# Plot the learning curves!\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_sizes, train_scores_mean, label = 'training score')\n",
    "plt.plot(train_sizes, test_scores_mean, label = 'test score')\n",
    "plt.ylabel('r2 score', fontsize = 14)\n",
    "plt.xlabel('Training set size', fontsize = 14)\n",
    "plt.title('Learning curves', fontsize = 18, y = 1.03)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **>>>** Looking at the new learning curves, how many training examples are sufficient for the model to learn the patterns of the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here!\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below after you've come up with the answer to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the learning curves\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_sizes, train_scores_mean, label = 'training score')\n",
    "plt.plot(train_sizes, test_scores_mean, label = 'test score')\n",
    "plt.ylabel('r2 score', fontsize = 14)\n",
    "plt.xlabel('Training set size', fontsize = 14)\n",
    "plt.title('Learning curves', fontsize = 18, y = 1.03)\n",
    "\n",
    "# Plotting a line where difference of train and test score becomes <1%\n",
    "plt.axvline(1400, linestyle='--', c='black')\n",
    "plt.annotate('Past this line:\\ntrain_score - test_score <= 0.01', xy=(1450, 0.7))\n",
    "\n",
    "\n",
    "# Comparing test scores at that line and at max training data (80% of data)\n",
    "plt.scatter(train_sizes[14], test_scores_mean[14], c='orange', s=50)\n",
    "plt.annotate(\n",
    "    f\"R2: {round(test_scores_mean[14], 2)}\",\n",
    "    xy=(train_sizes[14] + 50, test_scores_mean[14] - 0.03),\n",
    "    fontsize=12, c='orange'\n",
    ")\n",
    "\n",
    "plt.scatter(train_sizes[31], test_scores_mean[31], c='orange', s=50)\n",
    "plt.annotate(\n",
    "    f\"R2: {round(test_scores_mean[31],2)}\",\n",
    "    xy=(train_sizes[31] - 200, test_scores_mean[31] - 0.03),\n",
    "    fontsize=12, c='orange'\n",
    ")\n",
    "\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ÑπÔ∏è The more data, the longer the training. In certain cases, you will be working with enormous datasets. In those situations, the learning curves can help you find the right trade-off between reducing the training size (and training time!) while maintaining a high-performing model.\n",
    "\n",
    "The score at `train_size=1500` is nearly the same as with the full dataset! On the other hand, you could have reduced the computational expense - imagine saving 60% of a 1TB dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Model Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen the evolution of $R^2$ score of our models, but let's see **how accurately the models predict**.\n",
    "\n",
    "‚ùì **>>>** Calculate the MSE - *Mean Squared Error* (mean of `predictions - target`) - for two models trained on the **full** dataset:\n",
    "\n",
    "1. A model trained on all the features - `mp`, `poss`, pacing and a player's defense/offense ratio\n",
    "2. A model trained with `degree=2` polynomial features\n",
    "\n",
    "‚ùì **>>>** Save the resulting MSE's into variables called `reg_score` and `poly_score` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ **Expected results** : Regular model MSE: 4.36 and Degree-2 polynomial feature model MSE: 1.53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
