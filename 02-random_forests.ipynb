{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a698ab94-a0c7-4ce1-9e39-dbe5748c95f9",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b298800-84fc-4463-b282-6ebc9b8386f2",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "Decision trees require us to make a difficult decision: a **deep tree** with many leaves will be **too accurate** because each prediction will have been calculated on a limited number of houses. However, a **shallow tree** with few leaves will be **less performant** because it fails to capture the importance of different features in our dataset.\n",
    "\n",
    "Even the most sophisticated modeling techniques today face this trade-off between underfitting and overfitting. However, many models have clever ideas that can lead to better performance, such as the ***random forest*** algorithm.\n",
    "\n",
    "A ***random forest*** generates many different decision trees and makes a prediction by **averaging the predictions of each tree**. Its predictions are usually much better than those of a single decision tree, and default parameters are often sufficient.\n",
    "\n",
    "Disadvantage: the algorithm acts like a **black box**, and it's difficult to explain its choices.\n",
    "\n",
    "This algorithm takes various forms. It can be used to predict a **discrete value** (*regressor*) or predict a **class** (*classifier*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448dea46-488d-4cfd-8638-2b36477a3b5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "df = pd.read_csv(\"data/iowa_housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc902fe3-e700-4a99-a881-4e0fdd7a1bb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = df['saleprice']\n",
    "\n",
    "feature_names = ['lotarea',\n",
    "                 'yearbuilt',\n",
    "                 '1stflrsf',\n",
    "                 '2ndflrsf',\n",
    "                 'fullbath',\n",
    "                 'bedroomabvgr',\n",
    "                 'totrmsabvgrd',]\n",
    "\n",
    "X = df[feature_names]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875a5a5b-9f0b-43fe-a5c9-86abd8075ce1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "iowa_rf_model = RandomForestRegressor(random_state=42)\n",
    "iowa_rf_model.fit(X_train, y_train)\n",
    "y_pred = iowa_rf_model.predict(X_test)\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4213080-0ff5-4e3a-8938-db16ebf622e8",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "# More metrics\n",
    "\n",
    "So far we used the MAE as or main metric. But we could use other metrics to test our predictions.\n",
    "\n",
    "### MSE (*Mean Squared Error*)\n",
    "\n",
    "The MSE (*Mean Squared Error*) quantifies the average squared difference between the predicted values and the actual values in a dataset. It penalizes larger prediction errors more heavily due to the squaring operation. On the other hand the unit is now very different from the one used for the prediction, so it's not easy to compare it to the original unit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ca2cee-dbd0-4681-9efb-fa9ef303526b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div>\n",
    "<img src=\"files/mse_formula.svg\" alt=\"MSE\" width=\"20%\" align='center'/> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1406e4f-7bb6-4c84-b3a3-7624f0ef4f2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab9d8d1-84c2-483b-9a54-1584592085ee",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### R², coefficient of determination\n",
    "\n",
    "The proportion of the variance in a dependent variable (y) that is explained by one or more independent variables (X) in the regression model.\n",
    "\n",
    "R² is a value between 0 and 1, where:\n",
    "\n",
    "- **R² = 0** indicates that the model does not explain any of the variance in the dependent variable, and it's essentially a **poor fit**.\n",
    "- **R² = 1** indicates that the model perfectly explains all the variance in the dependent variable, and it's an **excellent fit**.\n",
    "\n",
    "\n",
    "The formula for calculating R² is as follows:\n",
    "```\n",
    "R² = 1 - (MSE_model / MSE_baseline)\n",
    "```\n",
    "Let's say we have only two values in our dataset, and we made those predictions :\n",
    "\n",
    "**- Actual values: (10, 15)**\n",
    "\n",
    "**- Predicted values: (12, 13)**\n",
    "\n",
    "First, let's calculate the **mean** of the actual values:\n",
    "```\n",
    "Mean of actual values = (10 + 15) / 2\n",
    "                      = 12.5\n",
    "```\n",
    "Second, let's compute the **Mean Squared Error (MSE)** of our model:\n",
    "\n",
    "```\n",
    "MSE_model = ((10 - 12)² + (15 - 13)²) / 2\n",
    "          = (4 + 4) / 2\n",
    "          = 4\n",
    "```\n",
    "Now, let's calculate the MSE of a simple baseline model. This is done by using the mean of the actual values instead of making different predictions for all our data points. In this case, the mean of actual values is 12.5, so:\n",
    "\n",
    "```\n",
    "MSE_baseline = ((10 - 12.5)² + (15 - 12.5)²) / 2\n",
    "             = (6.25 + 6.25) / 2\n",
    "             = 6.25\n",
    "```\n",
    "We've got everything we need, so lets' use the formula to calculate R²:\n",
    "\n",
    "```\n",
    "R² = 1 - (MSE_model / MSE_baseline)\n",
    "   = 1 - (4 / 6.25)\n",
    "   = 1 - 0.64\n",
    "   = 0.36\n",
    "```\n",
    "\n",
    "R² value is 0.36, meaning our model explains 36% of the variance in the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d3640-640a-41b4-b94a-a39eb95b3fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# r2_score([10, 15], [12, 13]) # Our example earlier\n",
    "\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb178ded-eec3-46cc-9fc9-34cc44ce016b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The method .score() from our model already uses the R² as metric.\n",
    "iowa_rf_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced63bf0-a427-434d-aa5b-d2433f51bcbb",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Hyperparameters optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eccbd8e-27c6-444c-9eff-9b688f08f750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's display the hyperparameters of our model\n",
    "\n",
    "iowa_rf_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92865bf-426f-4271-b744-367489af0021",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6901432-452e-4c5c-9de0-fd34c374c717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = [1, 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num=11)]#.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e130a0a6-b886-4a15-8eb0-9836654c70c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's compute how many possibilities we have...\n",
    "possibilities = 1\n",
    "for param_list in random_grid.values():\n",
    "    possibilities *= len(param_list)\n",
    "possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41859961-ed0c-4f99-90d6-291ee0891d53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 10 different combinations (out of the 3960), and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator=iowa_rf_model,\n",
    "                               param_distributions=random_grid,\n",
    "                               n_iter=10,\n",
    "                               cv=4,\n",
    "                               verbose=2,\n",
    "                               random_state=42,\n",
    "                               n_jobs=-1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebeca3c-7a70-49b3-ac2e-1f93437bbd6b",
   "metadata": {},
   "source": [
    "## Cross validation (K-Fold)\n",
    "<div>\n",
    "<img src=\"files/cross_validation.jpg\" alt=\"cross_validation\" width=\"70%\" align='center' source=\"https://www.50a.fr/img/upload/machine%20learning..jpg\" /> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3415eaa6-35c5-4e0c-accf-d7e249805a9a",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "Cross-validation is a resampling method that uses different portions of the data to test and train a model on different iterations. At the end, the model will have been trained on the entire dataset, and the average score of each iteration is calculated. This helps to better train the model, especially in cases where data is limited.\n",
    "\n",
    "## Random search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22994aa-a3d5-41f9-a811-e585319bdb4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The best random forest model found\n",
    "\n",
    "rf_random.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9a799f-b19d-49ce-8dbb-81ed37ad239c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Its parameters\n",
    "\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c1f447-b28d-4704-bc42-4a0110765dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The best parameters according to the random search\n",
    "rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e38bbc-4404-4b7a-9ef1-d377390562f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The score using the best parameters\n",
    "\n",
    "rf_random.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb6bb25-c909-46a8-9a6f-6934021c5d5f",
   "metadata": {},
   "source": [
    "### *Fine Tuning* : Grid Seach CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207e3540-34b2-4a1d-bf26-fc324c32b9c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These are good parameters found thanks to the random search\n",
    "\n",
    "{'n_estimators': 1400,\n",
    " 'min_samples_split': 5,\n",
    " 'min_samples_leaf': 1,\n",
    " 'max_features': 'sqrt',\n",
    " 'max_depth': 30,\n",
    " 'bootstrap': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac685fff-cac4-4866-99f5-81bc3e796835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Let's create the parameter grid based on the results of random search\n",
    "# (results may vary, because it's a random search!)\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [20, 30, 40],\n",
    "    'max_features': ['sqrt'],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_samples_split': [3, 5, 7],\n",
    "    'n_estimators': [1200, 1400, 1600]\n",
    "}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search_rf = GridSearchCV(estimator=iowa_rf_model,\n",
    "                              param_grid=param_grid, \n",
    "                              cv=2,\n",
    "                              n_jobs=-1,\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d4d117-7717-4f35-9182-a727a7ccaa7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_search_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b31b73-1e7b-4049-9d72-506e66562935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_search_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d0fbc6-93bd-4d3d-8e90-39fccdab0507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_search_rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f9f7a3-3bec-40f7-92c5-80b875f6d398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Same thing then :\n",
    "grid_search_rf.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95007136-db79-4a22-9f05-cd73debfb33b",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5790e3e-366b-41e9-9335-e95a4ebedbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "from joblib import dump, load\n",
    "dump(final_rf, 'best_iowa_rf_model.joblib') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
