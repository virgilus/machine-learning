{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26326b03-54cd-4e4c-ac5e-d5a59711dbcc",
   "metadata": {},
   "source": [
    "# Performances Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02192758-3368-404e-ad2b-1b78fc11d815",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n",
    "\n",
    "Let's see a new model which is capable of solving both regression or classification problems : K-Nearest Neighbors (KNN).\n",
    "\n",
    "### Principle\n",
    "\n",
    "<div>\n",
    "<img src=\"files/knn_1.webp\" width=\"65%\" align='center' source=\"https://medium.com/swlh/k-nearest-neighbor-ca2593d7a3c4\"><br></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c91660-03bb-450f-81b6-04eb5c04578f",
   "metadata": {},
   "source": [
    "When we use the model for **regression**, the result is the mean of the $k$ nearest neighbours. You'll notice that $k$ is called a hyperparameter, it's up to the human behind the screen to choose it.\n",
    "\n",
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6137833-dc3f-4a70-84ae-f0a535683a77",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div>\n",
    "<img src=\"files/knn_2.webp\" width=\"65%\" align='center' source=\"https://medium.com/swlh/k-nearest-neighbor-ca2593d7a3c4\"><br></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea217424-38b0-44a0-ae7e-01c9d7d5e3b0",
   "metadata": {},
   "source": [
    "### Distance\n",
    "\n",
    "There are several ways to compute the distance between two different points (Manhattan, Hamming, Jaccard, Cosine etc.). But default is juste the euclidian distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f9bab4-ecd0-43be-9f98-c27b6c6c179c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div>\n",
    "<img src=\"files/knn_3.webp\" width=\"45%\" align='center' source=\"https://medium.com/swlh/k-nearest-neighbor-ca2593d7a3c4\"><br></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeb21a7-1296-4f43-8c7a-53d0db97de6d",
   "metadata": {},
   "source": [
    "## $k$ : the number of neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c833781-bafa-4405-b271-5dc22959f5d7",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"files/knn_4.webp\" width=\"45%\" align='center' source=\"https://medium.com/swlh/k-nearest-neighbor-ca2593d7a3c4\"><br></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518a3b46-ef3e-4d8f-8e38-44443bcbb466",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "To enhance our models we need to compare the results to something. This \"something\" is called a baseline, it is usually a simple model. Once we've chosen our baseline, we can start using many other models and see if they perform better or worse.\n",
    "\n",
    "## Metrics\n",
    "To assess the performance of a model, we need metrics and they are many different types of metrics in Machine Learning. Each one of them has pros and cons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e98cbb-87fd-49b4-9256-5c1426e05bf3",
   "metadata": {},
   "source": [
    "### About the Data\n",
    "\n",
    "Our **target**:\n",
    "\n",
    "- **charges**: Individual medical costs billed by health insurance.\n",
    "\n",
    "Our **features**:\n",
    "\n",
    "- **age**: Age of primary beneficiary.\n",
    "- **sex**: Insurance contractor gender: female, male.\n",
    "- **bmi**: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9.\n",
    "- **children**: Number of children covered by health insurance / Number of dependents.\n",
    "- **smoker**: Whether the contractor is a smoker or not. .\n",
    "- **region**: The beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1646b924-6764-4e5f-944a-1b4ce9391d0a",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f047631-94d7-4cca-a740-7fc9c484338f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d27b62-81a1-44ba-9576-d8fe5a908ad2",
   "metadata": {},
   "source": [
    "## Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fef375e-dd7e-4d7b-89d2-2e4e9c3a5866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/insurance.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888080a9-c265-447e-956e-da320e09c21b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df['price_range_encoded'] = LabelEncoder().fit_transform(df['price_range'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5babd73f-ac7e-4b7a-a3c8-ddd718bc8335",
   "metadata": {},
   "source": [
    "### Holdout Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeac827b-5aae-48a8-8714-f8a7692d8337",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[['age','bmi','children','smoker']]\n",
    "y = df['charges']\n",
    "\n",
    "# Before doing anything, let's to make sure we don't leak information\n",
    "# So we're creating a train test right away, before altering the data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42) # Holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb01594-d5a3-4ae8-99aa-d742bff4d84a",
   "metadata": {},
   "source": [
    "### Dummy Regressor\n",
    "\n",
    "A dummy regressor is a very simple model. This one predicts the y using the mean of our target.\n",
    "\n",
    "[sklearn doc](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30672dd2-dabc-49f2-a8b5-70929e991923",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "baseline_model = DummyRegressor(strategy=\"mean\") # Baseline\n",
    "baseline_model.fit(X_train, y_train) #\n",
    "baseline_model.score(X_test, y_test) # R²"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cb0840-d246-433d-9c8e-8aafc26f8a56",
   "metadata": {},
   "source": [
    "The score is negative, so it's worse than than the mean. Why? Because we used the mean of our train dataset and we applied it to our test dataset. This score is the score we want to beat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fe8386-5741-4b31-8db3-4af2f55ae9bf",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4472936-c85b-4ce7-be9a-59d7128ca7c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_model = LinearRegression().fit(X_train, y_train)\n",
    "lr_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d676d4-3a51-4892-9831-fce8f85b126d",
   "metadata": {},
   "source": [
    "## Regression Error Metrics\n",
    "\n",
    "The goal of a regression model is to minimize the error. There are different ways to compute what we called \"error\".\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "Mean of the squared differences between ground truth and predicted values. Not expressed in the unit than the target.\n",
    "\n",
    "**Use MSE when**:\n",
    "\n",
    "- Larger errors have a disproportionally bigger impact, hence should be more penalized\n",
    "- Example: clinical trials, where an error of 4mg can be more than twice as bad as an error of 2mg\n",
    "- Direction and unit of error does not matter\n",
    "- Comparing the sensitivity of different models/methods to large errors\n",
    "\n",
    "### Root Mean Squared Error (RMSE)\n",
    "\n",
    "$RMSE = \\sqrt{MSE}$\n",
    "\n",
    "Square root of the Mean Square Error.\n",
    "\n",
    "**Use RMSE when**:\n",
    "\n",
    "- You want the MSE to be represented in the unit of the target, making it more interpretable.\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "$MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$\n",
    "\n",
    "The mean of the absolute differences between true values and predicted values.\n",
    "\n",
    "**Use MAE when**:\n",
    "- Errors can be penalized proportionally to their size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ce2554-606b-4b86-abe1-ff93fa480032",
   "metadata": {},
   "source": [
    "### Cheat sheet\n",
    "\n",
    "| Metric   | Advantages                                      | Disadvantages                                      | Example                                                                |\n",
    "|----------|-------------------------------------------------|----------------------------------------------------|------------------------------------------------------------------------|\n",
    "| MSE/RMSE | Highlights large errors, smooth, optimizable    | Sensitive to outliers                               | Accepts 5 errors of 1°C more than a single error of 5°C                  |\n",
    "| MAE      | Homogeneous, interpretable                      | Less sensitive to outliers                               | 5 errors of 1°C are equivalent to a single error of 5°C                  |\n",
    "| Max Error | The limit of the magnitude of an error is a priority | Case specific usage | A piece of equipment overheats if temperature goes over 2°C |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea18f75d-1b0e-44d4-8358-3b171121edd6",
   "metadata": {},
   "source": [
    "### Coefficient of determination (R²) and error metrics\n",
    "\n",
    "#### A quick reminder:\n",
    "\n",
    "  - $SS_{res} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "  - $SS_{tot} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2$\n",
    "  - $R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$\n",
    "\n",
    "#### Should we also use the R²?\n",
    "\n",
    "The R² has two specificities:\n",
    "\n",
    "- It facilitates the comparison between different models. Stating that a model has an MSE of 25 does not allow us to conclude if the model is correct because it depends on the values taken by the variable to be predicted.\n",
    "    On the other hand, the normalization done in the R² allows us to say that a model with less than 20% R² is not performing well, and conversely, a model that achieves more than 80% R² is performing well.\n",
    "\n",
    "- However, it is not very interpretable and does not provide information on the average error of the model. Indeed, while the R² allows us to compare the model's performance with a basic performance, it does not allow us to determine the average error made in predictions. It often needs to be combined with other metrics to better understand the model's performance, such as MSE or MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c30448b-c0cf-44e0-b13c-a18a4ce43ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error, max_error, r2_score\n",
    "\n",
    "# Fit and Predict\n",
    "lr_model = LinearRegression().fit(X, y)\n",
    "y_pred = lr_model.predict(X) # Let's compute on ALL our data. Because we want to compare different models.\n",
    "\n",
    "# MSE\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "\n",
    "# RMSE\n",
    "rmse = root_mean_squared_error(y, y_pred)\n",
    "\n",
    "# MAE\n",
    "mae = mean_absolute_error(y, y_pred)\n",
    "\n",
    "# Max Error\n",
    "max_error = max_error(y, y_pred)\n",
    "\n",
    "# R²\n",
    "rsquared = r2_score(y, y_pred)\n",
    "\n",
    "print('MSE =', round(mse, 2))\n",
    "print('RMSE =', round(rmse, 2))\n",
    "print('MAE =', round(mae, 2))\n",
    "print('Max Error =', round(max_error, 2))\n",
    "print('r² =', round(rsquared, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae8dffd-0685-4f80-904c-eb193275992a",
   "metadata": {},
   "source": [
    "### How well did we perform?\n",
    "\n",
    "Our model is trying to predict the medical costs billed by health insurance for each individual. This could be useful for an insurance company that would want to adjust the cost of the health insurance.\n",
    "\n",
    "The RMSE is about 6000 US dollars, but the MAE, less sensitive to outliers, is about 1/3 less at around 4200 US dollars.\n",
    "\n",
    "### One or several metrics?\n",
    "\n",
    "Should we compute all of the metrics above each time we fit a new model, or should we just pick one and try to improve it? It depends on what we're trying to achieve. But, at the start at least, it's a good practice to take a look at several metrics to make sure everything is ok.\n",
    "\n",
    "### Metrics and Cross Validation\n",
    "\n",
    "When we cross-validate a score, when we pass an argument to the ```scoring``` parameter to specify the metric. If it's empty, it's the default score. [You can find a list of the metrics here.](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "\n",
    "Many metrics have a \"neg_\" before their names. As they are treated as a score, sklearn assumes the higher it gets, the better. But, as we've seen previously, all our error metrics need to be lowered. So sklearn will take the negative value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a017c-2dd7-4f7a-b94b-9956c1c479cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "model = LinearRegression()\n",
    "cv_results = cross_validate(model, X, y, cv=5, \n",
    "                            scoring=['neg_mean_squared_error',\n",
    "                                     'neg_root_mean_squared_error',\n",
    "                                     'neg_mean_absolute_error',\n",
    "                                     'max_error',\n",
    "                                     'r2',])\n",
    "\n",
    "pd.DataFrame(cv_results).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14070377-8b7a-4651-999b-048b628ffc0a",
   "metadata": {},
   "source": [
    "## Classification Metrics\n",
    "\n",
    "### Correct and Incorrect Predictions\n",
    "\n",
    "When we use classification metrics, we don't output a number but a class. Let's say we only have two classes to predict : **True** or **False**. \n",
    "\n",
    "Therefore if we make a **correct** prediction we can have:\n",
    "\n",
    "- **True Positive**: A positive sample which has been **correctly** predicted as positive. The value is **True** in both ```y``` and ```y_pred```.\n",
    "- **True Negative**: A negative sample which has been **correctly** predicted as negative. The value is **False** in both ```y``` and ```y_pred```.\n",
    "\n",
    "But if we make an **incorrect** prediction we can have:\n",
    "\n",
    "- **False Positive**: A negative sample which has been **incorrectly** predicted as positive. The value is **False** in ```y``` but **True** in ```y_pred```.\n",
    "- **False Negative**: A positive sample which has been **incorrectly** predicted as negative. The value is **True** in ```y``` but **False** in ```y_pred```.\n",
    "\n",
    "In a multiclass problem there is one score for each class, counting any other class as a negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df152aea-b0aa-4e04-9280-f15f5b90148c",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "<div>\n",
    "<img src=\"files/confusion_matrix.jpg\" width=\"40%\" align='center' source=\"https://medium.com/analytics-vidhya/what-is-a-confusion-matrix-d1c0f8feda5\" /> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28aa82e-b17f-4861-a4ba-b9e3b093467f",
   "metadata": {},
   "source": [
    "### Confusion Matrix using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82f522c-76c5-49f2-a1b2-301e3294af3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1] # actual truths\n",
    "y_pred = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1] # predictions\n",
    "\n",
    "results_df = pd.DataFrame({\"actual\": y_test,\n",
    "                           \"predicted\": y_pred}) #Store results in a dataframe\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f35a738-972b-4982-b2c4-d7770d996987",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.crosstab(index=results_df['actual'],\n",
    "            columns=results_df['predicted']).reindex(index=[1, 0], columns=[1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44df7ce-8cf2-4615-b12c-8390522e10f1",
   "metadata": {},
   "source": [
    "### Confusion Matrix using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1579155e-7f98-4a3f-97d8-b9a6b25799e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_test = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1] # actual truths\n",
    "y_pred = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1] # predictions\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred,)[::-1, ::-1]\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2153b553-8ee1-45a0-9974-e13a38ee1f95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=[1, 0]).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9761afa-3caf-4b84-9edf-012057877962",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Accuracy\n",
    "\n",
    "Sum if the correct predictions divided by the sum of the overall number of predictions.\n",
    "\n",
    "$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$ or $Accuracy = \\frac{Number\\ of\\ Correct\\ Predictions}{Total\\ Number\\ of\\ Predictions}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52b53de-b0ab-4d8f-b790-fc162b439311",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Accuracy Score for our y_test and y_pred\n",
    "# [4, 1] [TP, FN]\n",
    "# [2, 3] [FP, TN]\n",
    "\n",
    "(4 + 3) / (4 + 3 + 2 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dcc538-a2b4-45ed-b8a5-d86762d9f1d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7464bb2a-e99b-42a1-89bb-12f02b81ad42",
   "metadata": {},
   "source": [
    "### Accuracy limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cd62ee-4fba-4209-8fe7-f3a56e14a833",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = np.array([[1, 0],\n",
    "              [9, 90]])\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=a, display_labels=[1, 0]).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4443c48e-e2ed-46d7-a903-c92406074806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Accuracy Score\n",
    "(1+90) / (1+0+9+90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f30255-980a-409e-acae-56a1be0d895a",
   "metadata": {},
   "source": [
    "Our model seems to predict very well! But... Actually our data is very imbalanced and it failed to predict the positive class.\n",
    "\n",
    "So accuracy is good when:\n",
    "\n",
    "- Target are balanced\n",
    "- Prediction of each class is equally important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b73b3a-6b23-4d55-b4ae-be8486419e9f",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "The recall, also known as sensitivity or true positive rate (TPR), measures the proportion of actual positive cases that were correctly identified by the classifier.\n",
    "\n",
    "$Recall = \\frac{TP}{TP + FN}$ or $Recall = \\frac{True\\ Positives}{True\\ Positives + False\\ Negatives}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6856ec-6508-41b0-be14-816b16ad56d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recall Score for our y_test and y_pred\n",
    "# [4, 1] [TP, FN]\n",
    "# [2, 3] [FP, TN]\n",
    "\n",
    "4 / (4+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30584369-f803-42ed-b480-12526342e29b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4b953f-f4aa-4f98-b52d-0a4e391b67cb",
   "metadata": {},
   "source": [
    "#### Recall Advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967b1b9e-08d7-4d2c-b158-990cc4312a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Previous example\n",
    "\n",
    "a = np.array([[1, 0],\n",
    "              [9, 90]])\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=a, display_labels=[1, 0]).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d580e5-1b8c-4072-ae89-6428b63d5fec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recall for a\n",
    "\n",
    "1 / (9+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc952089-8c22-4147-b201-928a175ba6f8",
   "metadata": {},
   "source": [
    "Now, thanks to the recall metric, we can state that this model is very bad at identifying the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb459c4-3fba-4890-a9da-4882e541deb7",
   "metadata": {},
   "source": [
    "#### Recall Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c5a71-ff44-4859-a5ea-ba3f4c10c091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b = np.array([[27, 100],\n",
    "              [3, 10]])\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=b, display_labels=[1, 0]).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e9ac07-86da-4840-9685-37cd67b08055",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recall for b\n",
    "\n",
    "27 / (27 + 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80ec4fd-6521-4d74-a444-c4d5675606f1",
   "metadata": {},
   "source": [
    "To improve the recall score we need to lower the number of false negatives. No matter how many false positive or true negatives you have, it doesn't change the recall score.\n",
    "\n",
    "With the result for the array b, the recall is high so the model did a good job at predicting if a sample is positive. But we also got a lot of fake positives (false alarms).\n",
    "\n",
    "Recall is good when:\n",
    "\n",
    "- It is important to identify as many occurrences of a class as possible, reducing false negatives but potentially increasing false positives\n",
    "- You don't want to miss any positive classes (E.g. Detecting fraudulent transactions, cases of a novel disease or potential sales leads)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ba896a-446a-425a-87bb-150886b80530",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "Measures the ability of a model to avoid false alarms for a class.\n",
    "\n",
    "$Precision = \\frac{TP}{TP + FP}$ or $Precision = \\frac{True\\ Positives}{True\\ Positives + False\\ Positives}$\n",
    "\n",
    "We take a look at the column of predicted values and look at the relation between true positives and false positives.\n",
    "\n",
    "Precision is good when:\n",
    "\n",
    "- It is important to be correct when identifying a class we don't want false positive. Ex: target advertising, we want to make sure that the customer is someone who potentially needs our product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a36415-2ca6-49d5-b2c2-5554f44cfa95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Precision Score for our y_test and y_pred\n",
    "# [4, 1] [TP, FN]\n",
    "# [2, 3] [FP, TN]\n",
    "\n",
    "4 / (4+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f754377-ba4c-4a7a-9581-de903b22cca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb3e65c-1368-4080-b583-51f672df24e2",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall.\n",
    "\n",
    "$F1\\ Score = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$\n",
    "\n",
    "It comes from the F-beta score. We decide that precision and recall are weighted the same, but we could choose an other one (F2 or F0.5).\n",
    "F1-Score is influenced more by the lower of the two values. Usually precision and recall are a balance. If you improve precision, recall score will lower.\n",
    "\n",
    "F1 Score is good when:\n",
    "\n",
    "- You want a general metric to compare across models and datasets.\n",
    "- You want to combine the Precision/Recall tradeoff in a single metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be957e5-ec1c-41fc-abb5-cbf3baed21d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# F1 Score for our y_test and y_pred\n",
    "# [4, 1] [TP, FN]\n",
    "# [2, 3] [FP, TN]\n",
    "\n",
    "# recall = 4 / (4+1)\n",
    "# precision = 4 / (4+2)\n",
    "\n",
    "2 * ((4 / (4+2) * (4 / (4+1)))) / (4 / (4+2) + (4 / (4+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7379c908-66c4-4c7d-b095-0dda4bb3ce42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4443c6fa-8b1f-4983-8d54-a13b7743e9e9",
   "metadata": {},
   "source": [
    "### An other way to visualize it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68451042-60ba-4281-9818-4ee935dff717",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div>\n",
    "<img src=\"files/precision_recall_wiki.png\" alt=\"precision_recall_accuracy\" width=\"40%\" align='center' source=\"wikipedia\" /> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00b998c-4a2b-4c99-95f5-21708f264337",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"files/precision_recall_accuracy.png\" alt=\"precision_recall_accuracy\" width=\"70%\" align='center' source=\"https://medium.com/@shrutisaxena0617/precision-vs-recall-386cf9f89488\" /> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13be3dbc-a9bc-453b-a56a-e4fa98daea22",
   "metadata": {},
   "source": [
    "## Which metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1344cc53-d328-4f45-a649-83845b4d9108",
   "metadata": {},
   "source": [
    "We're  building a model to detect the safety of seatbelts. What metric should we optimize for?\n",
    "- Seat belt safe = 1\n",
    "- Seat belt faulty = 0\n",
    "\n",
    "So we're trying to minimize the numbers of fake positives, because it might kill someone.\n",
    "\n",
    "Which metric should we look for?\n",
    "(Answer inside the hidden cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f004d57-3b89-4f99-bb27-b33fc500dd21",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Answer : Precision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
