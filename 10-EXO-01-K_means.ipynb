{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that our data actually contains relevant clusters, we will generate it ourselves.\n",
    "\n",
    "To do so, we will use [`make_blobs`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) from `sklearn`.  \n",
    "\n",
    "We want a dataset with **500 observations**, **2 features** and **4 clusters**.  \n",
    "\n",
    "We use *random_state=42* so that you can compare results with your buddy.\n",
    "\n",
    "**Run the cell below to generate your data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "X, y = make_blobs(n_samples=500, centers=4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **>>>** Make a scatter plot of your two features against each other. Color the points according to their corresponding  value in `y`.\n",
    "\n",
    "Don't forget the color argument is:\n",
    " - `c` for matplotlib  \n",
    " - `hue` for seaborn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see 4 distinct clusters, each with a different color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that we never knew about `y` and only received `X` to work with.  \n",
    "We only have 2 features and no target.\n",
    "\n",
    "Your goal is to find the **number of clusters** ($k$) that best matches the structure of your data.  \n",
    "\n",
    "\n",
    "❓ **>>>** Import `KMeans` from `sklearn` and initiate a model with the parameters:\n",
    "- `n_clusters=2`,\n",
    "- `random_state=42`\n",
    "\n",
    "❓ **>>>** Fit the model on your `X`  \n",
    "❓ **>>>** Get your predictions and store them in a `y_pred` variable.\n",
    "\n",
    "**Hint**: Everything is just one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are a vector of cluster assignment for each observation.  \n",
    "With `n_clusters=2` each observation in `X` will be associated to either one of two clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **>>>** Using the previous code, make a scatter plot of your two features against each other.\n",
    "\n",
    "❓ **>>>** Color the points according the predicted cluster in `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can still see 4 distinct clusters, however the color only show 2.\n",
    "**This clustering around 2 centers is clearly no optimal, we can do better.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the optimal number of clusters $k$\n",
    "\n",
    "Once fitted, the `KMeans` instance gains an attribute named `inertia_`.\n",
    "\n",
    "It represents the **sum of squared distances of observations to their associated (closest) cluster center**. \n",
    "\n",
    "So the lower, the better.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "KMeans(n_clusters=2, random_state=42).fit(X).inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of this in comparison to the Sum of Squared Errors in a Linear Regression.\n",
    "\n",
    "- `SSE` of a `Linear Regression` --> `Sum of squared distances between observations and the regression line`  \n",
    "\n",
    "- `Inertia` of a `KMeans Clustering` --> `Sum of distances between observations and their closest centroid`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way for us to find the optimal number of clusters is a heuristic: the **Elbow Method**.  \n",
    "\n",
    "We have to try several number of clusters and look at the inertia obtained for each one.\n",
    "\n",
    "❓ **>>>** Fit a `KMeans` for every number of clusters between 1 and 10, for each one, save the inertia in a list `wcss` (Within-Cluster Sum of Square)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wcss = []\n",
    "clusters = list(range(1, 11))\n",
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **>>>**  Plot the inertias in `wcss` against their corresponding number of clusters ❓ **>>>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly see an Elbow at 4 clusters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means with optimal clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the optimal number of clusters know, it's time to fit a last `KMeans`.\n",
    "\n",
    "❓ **>>>** Fit a `KMeans` with `n_clusters=4` on your `X`, store the predictions in `y_pred`  \n",
    "\n",
    "❓ **>>>** Make a scatter plot of your two features against each other, and color the points  according the predicted cluster in `y_pred`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successfully identified **4 clusters** among our observations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** Scaling features before clustering is not always necessary, but it rarely hurts.\n",
    "You can check these [detailed answers](https://datascience.stackexchange.com/questions/6715/is-it-necessary-to-standardize-your-data-before-clustering) to go further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
