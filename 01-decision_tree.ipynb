{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e360a3c3-ca80-4e45-b5f1-7a6a5ce60fbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div>\n",
    "<img src=\"files/machine_learning.jpg\" alt=\"CPU\" width=\"100%\" align='center' source=\"https://www.50a.fr/img/upload/machine%20learning..jpg\" /> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cfa533-9fc1-4673-ac5d-9fe97a51ac4b",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Machine learning may seem intimidating with its jargon derived from the realms of computer science and statistics. However, if we start with the basics and progressively increase the complexity, it is entirely possible to grasp the fundamental concepts of this field.\n",
    "\n",
    "This course will provide you with an overview of how data scientists develop, design, and implement their ML models. You can then use this knowledge to continue learning on your own or stop now if you think you know enough to be able to talk with data scientists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd7d0f0-b3a0-4ccd-807e-65d971db9c99",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "# Practical Case: Real Estate\n",
    "\n",
    "The first dataset we will use contains data on the real estate. In real life, real estate agents can estimate the value of a property by associating a price with various characteristics of the property (number of rooms, area, location, etc.) based on their experience.\n",
    "\n",
    "The program we are going to create will allow us to make predictions ourselves, i.e., to predict a given value. However, this time it's the computer that will \"learn\" on its own thanks to the data we will provide.\n",
    "\n",
    "## Decision Tree\n",
    "\n",
    "For now, we will use a model called a \"decision tree.\" These are very intuitive models, easy to understand and analyze, which can be useful in many cases.\n",
    "\n",
    "Let's start with a very simple example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5235ae6d-e4f9-472e-a565-d2546258e25b",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"files/decision_tree_1.png\" alt=\"CPU\" width=\"50%\" align='center'/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441e0641-3606-4922-828b-6a9f3cd2eb4c",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "This model divides houses into two categories: those with 2 bedrooms or less and those with more than 2 bedrooms, and then it displays the average price of each group.\n",
    "\n",
    "The model uses the dataset to decide how to allocate houses into these two groups, and then again to predict the price within each group. The step of setting a model's parameters from data is called training or fitting. The data used to set up this model is called training data.\n",
    "\n",
    "The details of how the model is trained (e.g., how to split the data) are quite complex, and we will not discuss this topic in these notebooks. Once the model is fitted, we can apply it to new data to predict the price of a home.\n",
    "\n",
    "We can consider more factors by using a tree with more \"splits,\" meaning it is \"deeper.\"\n",
    "A decision tree that also takes into account the size of each house's land might look like this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6c028f-67d7-4e09-a1a4-c2b3ea3a1d9c",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"files/decision_tree_2.png\" alt=\"CPU\" width=\"60%\" align='center'/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd63fc8-3337-45a1-a4a6-ab1b9ba84106",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "To predict the price of a house, we go through the decision tree, always choosing the path that corresponds to the features of that house. The predicted price for the house is found at the bottom of the tree, and this point is called a leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab32871e-42dc-4481-8655-78b12c7bae2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div>\n",
    "<img src=\"files/classifier_tree_meme.webp\" alt=\"CPU\" width=\"50%\" align='center' /> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecce1c7-d51d-4b8c-9334-926ecae6069a",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "# Exploration with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6b382c-66bb-4912-b441-2e80516b6ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/iowa_housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3428a86a-cea4-4262-a22b-649db5ca18f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1fd721-593a-4432-a360-d90ef5f07de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920b04a6-68da-4bd1-a9e7-5b84bc98496e",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ddd0f-a4b7-489d-bdd4-f86a00da3954",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d75276-8e67-42f0-b170-f4e13f0a11fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d40140-002d-42e0-9392-7a66011bf94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().loc[df.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ec27a8-0005-4420-b51c-0c0131ae6311",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8550dbca-a62b-43d5-898f-8e5cd6efec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_col_len = len(max(df.columns, key=len)) # Just to make sure that the table...\n",
    "max_val_len = len(str(max(df.isna().sum(), key=lambda x : len(str(x))))) # ...displays nicely :)\n",
    "\n",
    "for i, num in zip(df.isna().sum().index, df.isna().sum()):\n",
    "    print(f'{i}{(max_col_len - len(i)) * \" \"} | Missing values : {num}{(max_val_len - len(str(num))) * \" \"} | Completion : {round(100 - (num / df.shape[0] * 100))}%') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c678370-1a5c-41c5-809c-4e66ba381229",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bafbabd-28c8-406a-be5d-7db8a176e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lotarea'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a596594-747f-4f76-b1f5-058eb8d65f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lotarea'].mean().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa21f55-4b50-4eca-a233-18fa857a93bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['saleprice'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d62b92-a9a7-487e-aff8-d44dca528b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['saleprice'].mean().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1e0c76-5716-461b-93f7-4a8cdba0683e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d80f30e-0475-4911-af94-c420ead0755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['yearbuilt'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e0e1af-1ade-4b26-8a35-d8e19ea5bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['yearbuilt'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495056aa-52a1-42a9-bb52-3f9b1008ad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['yearbuilt'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af05700-f62c-45fd-b460-6a3117f0d03f",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "# Target Variable\n",
    "\n",
    "The **target variable**, also known as the response variable, dependent variable, the variable to predict, outcome variable or criterion variable is the variable we want to predict. It is represented by \"y\" (lower-case).\n",
    "\n",
    "In this case, it is the last column in our dataframe that contains the sale price of the real estate: `'saleprice'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d76d91-0700-4f4e-b92c-4bcdabfda5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['saleprice']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27bb1a2-a6a5-49cc-bf21-0db53fc18256",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "# Explanatory Variables\n",
    "\n",
    "The explanatory variables, also known as predictor variables or \"features\", are the input variables of our model. It is through these variables that the model will determine the value of our output variable. They are represented by \"X\" (upper-case).\n",
    "\n",
    "The choice of these variables has a significant impact on the results. Sometimes, we will use all the available variables, while other times we will only use a subset of them. There are many different methods (logical, scientific, statistical, computational, etc.) to help us make this choice.\n",
    "\n",
    "Here, we will use the following variables as features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700a8aa4-5b14-4025-b342-e6c348b448e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "'lotarea',\n",
    "'yearbuilt',\n",
    "'1stflrsf',\n",
    "'2ndflrsf',\n",
    "'fullbath',\n",
    "'bedroomabvgr',\n",
    "'totrmsabvgrd',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3142c48f-1303-4fb6-881b-b175e4ece843",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7186128-aed9-4584-ba9b-667daec62a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43744d58-5880-4058-bbae-407d8765cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792a9356-6b37-4a30-b5f1-3d898376fbac",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "# Modeling\n",
    "\n",
    "### Model Selection\n",
    "\n",
    "We will choose a \"decision tree,\" also known as *DecisionTreeRegressor*, which we will name 'iowa_model'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39125ad6-73f9-46e8-858a-329d7e50393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# random_state will allow model reproducibility\n",
    "iowa_model = DecisionTreeRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5c5326-4487-4479-81b5-b25f598dd10f",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Model Fitting\n",
    "\n",
    "Model training is very simple: just one line of code is enough! By convention, we first provide the features and then the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aed91b5-3bef-4c84-80d6-82e1dba76c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "iowa_model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6873e5-34bd-40f1-849a-bb4d56a1a230",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Visualization\n",
    "\n",
    "Once our model is created, we can visualize it in various ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa0bcbe-cb76-4e37-83b4-e7835d7da320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "# print(tree.export_text(iowa_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adbdc5b-4b71-4df0-a6e5-6c4bb3f3b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "tree.plot_tree(iowa_model,\n",
    "               feature_names=X.columns,\n",
    "               max_depth=2,\n",
    "               filled=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8028f0b7-66ed-4178-aeb9-e69261906aad",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Predictions\n",
    "\n",
    "Our model can now predict values based on a set of variables. Let's try having it predict the results based on the features (X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7881002-d333-4700-9807-588c1ea9a527",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = iowa_model.predict(X)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d67caef-6126-41e4-a37d-e0efb38a3c6d",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "Let's compare the first 5 predictions with the first 5 values of y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7636eb-8482-4b24-b06e-bd8f35b2abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(y_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc7247-200d-4301-98f3-330e0ddd8765",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75989df-b260-4ce2-8fa7-fa0c5c5cc621",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "It's strange, we would expect our model to be a bit off, but it seems like it's predicting our variable y down to the dollar! Let's verify this with some Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf15bfec-371f-4007-bbb7-9ee2ad978b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame({'y':y,'y_pred':y_pred})\n",
    "res['y_pred'] = res['y_pred'].astype(int)\n",
    "res['diff'] = res['y'] - res['y_pred'].round()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c2fb8e-3717-454e-90e2-9384f57b13fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.loc[res['diff'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f9270-9438-4eab-86ff-57919cf7a3b9",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "Out of the 2930 predictions made, only 71 are incorrect, and even those are not very far from the expected results. Have we created the best possible model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6315b8e-17d1-4418-b793-ebf61f669229",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "# Model Validation\n",
    "\n",
    "Each model, once trained, must be evaluated using different metrics. A good metric for evaluating a continuous value, as is the case here, is to examine the accuracy of the prediction. For each property, we will calculate the absolute difference between the actual value and the value predicted by the model:\n",
    "```\n",
    "error = |actual value - predicted value|\n",
    "```\n",
    "If we then take the average of these values, it gives us the MAE (Mean Absolute Error). The MAE tells us what the average difference is between a prediction and the actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8eb50d-a930-4449-b8f8-ddf499b7fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8052091-d1bf-4907-87bd-9a5153fe70bd",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "The accuracy is excellent, but that's because the way we adjusted our model is incorrect. Since the data we used to train and test the model are the same, it's normal that we get almost only correct answers. We are dealing with a classic case of **overfitting**.\n",
    "\n",
    "To evaluate the robustness of a model, we will test it on data it has never seen before. To do this, we will split our data into two groups: one will be used for training (*train*), and the other to test the model (*test*).\n",
    "\n",
    "The \"train_size\" parameter will determine the proportion of our data used for training. A value of 0.8 means that we reserve 80% of our data for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddfd64d-0ab7-487d-97cb-de343dbbd2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Now our data are split in 4 different parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size=0.8)\n",
    "\n",
    "iowa_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = iowa_model.predict(X_test)\n",
    "print(mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a0ce5c-2ebd-4360-8340-44c006744f2d",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "The MAE is significantly higher, approximately 200 times higher! Since the average price of a house was around $180,000, this means our model is off by about 1/6 of the price. There are, of course, many ways to achieve a higher score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077bdc9e-d0d0-4199-ae03-2df832ba4f39",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "# Model parameters\n",
    "\n",
    "A decision tree can be configured in many different ways, as you can see by examining the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) for our type of model.\n",
    "\n",
    "### Difference between parameters and hyperparameters\n",
    "\n",
    "In machine learning, hyperparameters are the parameters that govern the process of generating the internal parameters of the model.\n",
    "\n",
    "For example, in our model, its parameters include all the branches that lead to the leaves of our tree, among other things. These parameters were determined by the model during its training and changed significantly between the start of the fitting process and when it finished.\n",
    "\n",
    "Hyperparameters, on the other hand, are parameters that are often set by a human and are not modified during training. They represent high-level directions or settings.\n",
    "\n",
    "One of the most important hyperparameters for this model is the depth of the tree. For now, we didn't give any specific instructions, so this hyperparameter was generated by the program. Let's examine it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92188eb2-32ab-4d35-b589-30f06ed07551",
   "metadata": {},
   "outputs": [],
   "source": [
    "iowa_model.tree_.max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9873663-a86a-4647-832a-ec98ee22abe5",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "There is a maximum of 26 levels of depth in our tree. Each time we add a level of depth to our tree, we increase its maximum number of leaves and therefore its precision. However the number of houses in each leaf will be reduced, which means that predictions will become less reliable. So, we need to find a balance between precision and reliability.\n",
    "\n",
    "### Overfitting and Underfitting\n",
    "\n",
    "The phenomena of overfitting and underfitting are central concepts in machine learning.\n",
    "\n",
    "- Overfitting occurs when the model's results closely match the data it was trained on but make significant errors when applied to unknown data. This happens when our decision tree is too deep.\n",
    "\n",
    "- Underfitting occurs when the model fails to distinguish essential features in our data. It will have a poor score on both the training data and the test data. This occurs when our model is not deep enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf55b70-5c2e-4bbe-9f74-e18bd2c6c7d4",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"files/underfitting_and_overfitting.png\" alt=\"CPU\" width=\"75%\" align='center'/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cbaf92-3e09-4089-98de-2f30070c116e",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "The graph above shows the variation in MAE based on the depth of a decision tree. The term \"validation\" here refers to the \"test\" dataset. Here are some observations about the graph:\n",
    "\n",
    "- On average, the model will always have a better score when predicting data from its training set rather than unknown data from the test set.\n",
    "\n",
    "- Increasing the depth initially improves the model on both the training and test sets.\n",
    "\n",
    "- There comes a point where increasing the depth improves precision on the training set, but the MAE starts to increase on the test set. This is the phenomenon of overfitting.\n",
    "\n",
    "The goal of hyperparameters is to find this balance point, which should allow us to maximize the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55157dc9-6b1c-4a33-8d20-0c64d879fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae(max_leaf_nodes, X_train, X_test, y_train, y_test):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    return(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e6c5c-5c63-4825-8e65-d60051a759e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple\n",
    "get_mae(50, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13be26c2-56f8-4bc2-a6b3-3f753fb739b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for max_leaf_nodes in [2, 5, 25, 30, 40, 50, 100, 200, 400]:\n",
    "    mae = get_mae(max_leaf_nodes, X_train, X_test, y_train, y_test)\n",
    "    d[max_leaf_nodes] = mae\n",
    "    print(f\"Max leaf nodes: {max_leaf_nodes} \\t\\t Mean Absolute Error: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0876f2ea-11bd-4417-a576-1b4439d3816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame.from_dict(d.items())\n",
    "             .rename(columns={0 : 'max_leaf_nodes', 1 : 'mae'})\n",
    "             .plot(x='max_leaf_nodes', y='mae', color='red'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b62e6c0-3bbf-4d9c-8cca-e119214c915b",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "Graphically, the best hyperparameter seems to be around 40. We could automatically find it using certain techniques that we will explore later.\n",
    "\n",
    "### What happens next ?\n",
    "\n",
    "Once we have found the best hyperparameters, we can retrain the model but this time using the entire dataset to further improve accuracy. Then, we could test it on real data from a different dataset to see how it performs.\n",
    "\n",
    "Another possibility would be to use a different model and see if it performs better or worse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a499829-258a-4d5d-81bb-a12086cd75d2",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "# Let's go deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0d5e0a-3542-4d8d-9918-d6a609be5728",
   "metadata": {
    "tags": [
     "en"
    ]
   },
   "source": [
    "### Column Selection Based on the Rate of Missing Values\n",
    "\n",
    "Applying `df.dropna()` to all columns would delete all rows in our dataframe. However, we can choose to eliminate all columns with a completion rate lower than 94% (for example), which allows us to retain at least 94% of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae71787-0c87-4d73-93b4-2338055cfd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "(df['garagetype'].isna().sum() / df.shape[0]) < 0.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57ebcc5-dba5-4f7e-bd2c-38de57235048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2\n",
    "(df['miscfeature'].isna().sum() / df.shape[0]) < 0.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7872bcba-110b-4379-8be2-f0a1a2e7f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [col for col in df.columns if (df[col].isna().sum() / df.shape[0]) < 0.06]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf442389-b4be-4185-93e1-ed6ef9c6fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, df.dropna().shape, df[cols_to_keep].dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeb2159-1904-4017-a7c3-12ff7836dcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df[cols_to_keep].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e940e0-cc7a-436d-91b2-4541a76e4971",
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_df.isna().sum() > 0).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
