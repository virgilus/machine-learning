{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1045298d-5f4a-4138-9110-54c6782b5e1a",
   "metadata": {},
   "source": [
    "# Ensemble Method\n",
    "\n",
    "Ensemble methods in machine learning refer to techniques that combine the predictions of multiple individual models (often called base or weak learners) to produce a stronger model.\n",
    "\n",
    "The idea behind ensemble methods is to leverage the wisdom of crowds, where combining multiple weak models can often lead to better predictive performance than any individual model on its own.\n",
    "\n",
    "## Decision Tree\n",
    "\n",
    "We've already covered what is a decision tree. It can be used as a classifier or as a regressor. As a regressor the output in the mean of all samples in the leaf.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a99ac8e-5c68-4ca0-b2d0-33bee7cb0f41",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"files/decision_tree_2.png\" width=\"60%\" align='center'/> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48416d1c-ac3a-4395-8469-5556a3681b57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=np.c_[iris['data'], iris['target']],\n",
    "                  columns= iris['feature_names'] + ['target'])\n",
    "df.drop(columns=['sepal length (cm)', 'sepal width (cm)'], inplace=True) # Let's simplify the dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992025d7-c657-4f52-8b6c-3ab38ff278d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"petal length (cm)\", y=\"petal width (cm)\", hue=\"target\", palette=\"viridis\", s=70);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed427475-85ab-4588-9011-9313b6f1dd3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns=['target']).values\n",
    "y = df['target'].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8abc6f2-d155-4bf7-8c89-8c5723d047d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate and train model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "max_depth = 2\n",
    "tree_clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "tree_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a90bf7-7b83-411f-8d03-9034ba19b1c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "print(tree.export_text(tree_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8b96b3-0515-4945-bcad-48414ead1524",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "tree.plot_tree(tree_clf,\n",
    "               feature_names=df.drop(columns='target').columns,\n",
    "               max_depth=max_depth,\n",
    "               filled=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740df1a9-630a-456f-9806-012c8fbbbf8d",
   "metadata": {},
   "source": [
    "- The first \"box\" is called the \"root node\"\n",
    "- The next ones are the \"internal nodes\".\n",
    "- The ones with color are the \"leaves\" (prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea614c1-fc17-4df6-8fe3-5ce981a96e47",
   "metadata": {},
   "source": [
    "# Gini coefficient\n",
    "\n",
    "Also known as the Gini impurity, is a measure of impurity or disorder in a set of data points. In a Decision Tree context, the Gini index measures the ability of each feature to separate the data. Lower values of Gini impurity indicate less impurity and better splits.\n",
    "\n",
    "$Gini = 1 - \\sum_{i=1}^{n} (p_i)^2$\n",
    "\n",
    "$p_i$ being the ratio between the observations in a class and $i$ the total number of observations remaining at a given node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d678f31-4498-42d5-86f3-683630874dbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's compute the gini score for the root node\n",
    "1 - ((50 / 150)**2 + (50 / 150)**2 + (50 / 150)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe183681-2395-456b-859c-ce5a8b1a72b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's compute the gini score for the green box\n",
    "1 - ((0 / 54)**2 + (49 / 54)**2 + (5 / 54)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea0ae4f-e50a-44a6-889f-6217f5093104",
   "metadata": {},
   "source": [
    "### How Do We \"Grow\" a Tree?\n",
    "\n",
    "The tree's structure is defined through the following steps:\n",
    "\n",
    "1. Start at root node, which contains your entire dataset\n",
    "1. Try various combinations of **(feature, threshold)** combos; each would split your dataset into 2 child nodes\n",
    "1. For each combination, compute the weighted average Gini index of both child nodes (weighted by number of instances)\n",
    "1. Select the (feature, threshold) combo yielding the lowest index (i.e. the \"purest child nodes\")\n",
    "1. Split the dataset in two using this rule\n",
    "1. Repeat step 2 for both subsets\n",
    "1. Stop when no feature improves node impurity (at what risk?)\n",
    "\n",
    "So to find the root node, the algorithm tried many different combinations with our two features and different thresold, trying to lower the mean of the gini score of the two next leaves. That's how it found \"pental lenght (cm)\" and <= 2.45."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec6ece-edc8-4010-b51e-2dd763c34d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame(columns=X.columns, data=[[4, 1]]) # if X is a dataframe, you need to also provide a df\n",
    "\n",
    "tree_clf.predict([[4, 1]]) # 'petal length (cm)' and 'petal width (cm)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4ec00d-2ace-43d0-b1eb-15f75c4b426c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree_clf.predict_proba([[4, 1]]) # Not a true \"probability\", it's the ratio of classes in the node [0, 49, 5] for 54 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a468b06b-a34b-45a6-b96e-3f11a15f0afe",
   "metadata": {
    "tags": []
   },
   "source": [
    "This is not a score, so it's not easily interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2481b7a-0a9a-472b-bde8-a92f8e32e915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "plot_decision_regions(X, y, clf=tree_clf);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f46533-114d-4616-b62e-c6baac49d0e1",
   "metadata": {},
   "source": [
    "- The ```[4, 1]``` prediction is classified as a class 1.\n",
    "- The 90% previsously seen is the ratio of class 1 (49 / 54) on total sample in that area.\n",
    "- All separations are orthogonal to the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9940459-d3ba-4dc2-bc3e-99ff39a7b0fb",
   "metadata": {},
   "source": [
    "### Features importance\n",
    "\n",
    "Using a decision tree can help to decide which features are important. The Gini importance of a feature is calculated by summing the Gini impurity decrease (or the weighted impurity decrease) across all the nodes of the tree where the feature is used for splitting.\n",
    "\n",
    "- For each feature, the decision tree algorithm measures the decrease in Gini impurity that results from splitting on that feature.\n",
    "- The weighted impurity decrease across all nodes where the feature is used for splitting is then calculated.\n",
    "- Finally, the feature importances are normalized so that they sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592458c-e009-42ea-9403-dc9cf7705fb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4e6cd4-3784-409a-8bb5-077bbc2845ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decision trees VS other models\n",
    "\n",
    "<div>\n",
    "<img src=\"files/classifier_comparaison.png\" width=\"100%\" source='https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html' align='center'/> </div>\n",
    "\n",
    "[Sklearn Website](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1d0fa4-5452-4fd5-8351-4db77688d7b4",
   "metadata": {},
   "source": [
    "### Controlling Overfitting\n",
    "\n",
    "In order to have good results, decision trees must be tuned because default parameters will almost certainly overfit. You can control :\n",
    "\n",
    "- Split with the ```min_samples_split``` parameter.\n",
    "- Leaves with the ```min_samples_leaf``` parameter.\n",
    "- Tree depth with ```the max_depth``` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cbff10-fd91-4318-9efc-7318ef608e46",
   "metadata": {},
   "source": [
    "### Decision Trees : Pros and Cons\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "- No scaling necessary\n",
    "- Resistant to outliers\n",
    "- Intuitive and easy to interpret\n",
    "- Allows for feature selection (see Gini-based feature_importance_)\n",
    "- Non-linear modeling\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "- High variance (i.e. a small change in the data causes a big change in the tree's structure)\n",
    "- Long training time if grown up to (large) max depth.\n",
    "- Splits data \"orthogonally\" to feature directions\n",
    "- Use PCA upfront to \"orient\" data (see \"unsupervised learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a00b5b3-3806-42a5-9fb4-3d7ece3ac0b9",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "To understand what are random forests, let's first see a new concept : **bagging**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ff8e05-3f26-439e-8e7f-94aa0758c081",
   "metadata": {},
   "source": [
    "### Bagging (**B**ootstrap **Agg**regat**ing**)\n",
    "\n",
    "#### Bootstrapping\n",
    "\n",
    "Given a dataset of size $N$, a new dataset of the same size is created by randomly sampling $N$ examples from the original dataset with replacement. This means that each example in the original dataset can be sampled multiple times in the new dataset, or not at all.\n",
    "\n",
    "#### Aggregating\n",
    "\n",
    "This means we will train different models on each one of those new datasets and then combine them together. Some algorithms, like the random forest, also drop some features during the training process.\n",
    "\n",
    "<div>\n",
    "<img src=\"files/bagging.svg\" width=\"75%\" source='https://en.wikipedia.org/wiki/Bootstrap_aggregating' align='center'/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b5ab38-1a2d-4bfa-ae3e-dff55df3c5d8",
   "metadata": {},
   "source": [
    "- Bagging is also called a \"a parallel ensemble method\" and aims to reduce variance.\n",
    "- Each version of the model is called a **weak learner** (because each model is not very good on its own).\n",
    "\n",
    "<div>\n",
    "<img src=\"files/decision_tree_vs_random_forest.png\" width=\"75%\" source='https://towardsdatascience.com/from-a-single-decision-tree-to-a-random-forest-b9523be65147' align='center'/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c3f29c-a255-4380-9d20-5425b4d405df",
   "metadata": {},
   "source": [
    "## Using a random forest\n",
    "\n",
    "So Random Forests are a bagged ensemble of Decision Trees. Just as a classifier, predictions are averaged in regression tasks, and voted in classification tasks.\n",
    "\n",
    "Usually it's good to create many different trees (around 100), but not very deep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279ad630-b998-4a37-827b-1316744e7819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest = RandomForestRegressor(n_estimators=100,\n",
    "                               #random_state=42\n",
    "                              )\n",
    "\n",
    "cross_val_score(forest, X, y, scoring = \"r2\", cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b877155-70e9-4fb3-a7ff-1b878e153a42",
   "metadata": {},
   "source": [
    "### Bagging any algorithm\n",
    "\n",
    "You can use bagging on many different algorithms, not just trees!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f42214-60bb-44e7-a9df-22346be7683c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "knn_model.fit(X, y)\n",
    "plot_decision_regions(X, y, clf=knn_model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1645d73-94d2-479e-8292-afee25ed4e7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "weak_learner = KNeighborsClassifier(n_neighbors=4)\n",
    "bagged_model = BaggingClassifier(weak_learner, n_estimators=5)\n",
    "\n",
    "bagged_model.fit(X, y)\n",
    "plot_decision_regions(X, y, clf=bagged_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b30a6-279b-451b-9353-a2c926fa06e0",
   "metadata": {},
   "source": [
    "### Pros and Cons of Bagging\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "- Reduces variance (overfitting)\n",
    "- Can be applied to any model\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "- Complex structure\n",
    "- Long training time\n",
    "- Disregards the **performance of individual sub-models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa9867f-c031-4ec1-b42e-051fcfe021ff",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting is a method designed to train weak learners that learn from their predecessor's mistakes.\n",
    "\n",
    "- It is a sequential ensemble method. So it cannot use several processors at the same time.\n",
    "- The aim of boosting is to reduce bias.\n",
    "- Focuses on the observations that are harder to predict.\n",
    "- The best weak learners are given more weight in the final vote.\n",
    "\n",
    "<div>\n",
    "<img src=\"files/bagging_vs_boosting.png\" width=\"75%\" source='https://www.datacamp.com/tutorial/what-bagging-in-machine-learning-a-guide-with-examples' align='center'/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d05d06-990d-4257-8ebc-a66e1127aeca",
   "metadata": {},
   "source": [
    "### Adaboost (Adaptative Boosting)\n",
    "\n",
    "One of the most popular type of boosting is Adaboost. It can perform on several types of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6bf20d-2ca4-4f70-938f-31ce136c6678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), # The default tree in AdaBoost, but with the hyperparameter max_depth\n",
    "                               algorithm='SAMME',\n",
    "                               n_estimators=50)  \n",
    "\n",
    "model.fit(X, y)\n",
    "plot_decision_regions(X, y, clf=model);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde27138-780d-4431-a1e9-1b48f9d3c8fa",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "Gradient Boosting works only for trees, and is usually more performant than Adaboost. Instead of updating the weights of observations misclassified, it's going to predict for each next weak-learner what was misclassified by the previous model. So the trees become more and more complex with a lot of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75e3da8-76eb-461a-b8a8-417116534523",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=5000, max_depth=3)    \n",
    "model.fit(X, y)\n",
    "plot_decision_regions(X, y, clf=model);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2629ef82-64c3-47c0-bcee-66f55a4f792b",
   "metadata": {},
   "source": [
    "### XG Boost (Extreme Gradient Boosting)\n",
    "\n",
    "This algorithm is is known for its high performance and efficiency compared to traditional gradient boosting implementations. It can be used using a different library than sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef42143a-10f0-4a9e-a4f7-1bc732944605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier(n_estimators=5000, max_depth=3)\n",
    "\n",
    "model.fit(X, y)\n",
    "plot_decision_regions(X, y, clf=model);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad7342c-ae3a-45c9-b608-702a0fd6abf5",
   "metadata": {},
   "source": [
    "## Pros and Cons of Boosting\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "- Strong sub-models have more influence in final decision.\n",
    "- Reduces bias.\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "- Computationally expensive (sequential).\n",
    "- Easily overfits.\n",
    "- Sensitive to outliers (too much time spent trying to correctly predict them)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
