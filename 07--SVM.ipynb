{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15de7dae-ed58-4f4c-b478-4396d4d621f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "What's a good decision boundary for classification?\n",
    "\n",
    "There is an infinite number of potential decision boundaries that separate the classes (\"hyperplanes\").\n",
    "\n",
    "## Maximum Margin Classifier\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"files/SVM_1.webp\" width=\"55%\" source='https://medium.com/@senihberkay/svm-support-vector-machines-77c0e11f75f2' align='center'/>\n",
    "</div>\n",
    "\n",
    "- The hyperplane (lines) that generalizes best to unseen data is the one that is furthest from all the points (maximizes the margin).\n",
    "- The points on the margin boundary are called support vectors.\n",
    "- Finding them is a convex optimization problem (one single best solution).\n",
    "- This one is a **Maximum Margin Classifier** algorithm, it tries to find the largest margin.\n",
    "- SVM is very light to store in comparison to KNN.\n",
    "- We need features (X1, X2, X3 etc.) to be scaled.\n",
    "- A \"support vector\" is a datapoint which is used to draw the margin line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53410dda-64fc-4863-b046-d74e06ca406d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Soft Margin Classifier\n",
    "\n",
    "A SVM can very easily overfit and is very sensitive to outliers, that's why we use the **\"Soft Margin Classifier\"**. The Maximum Margin Classifier is almost never used.\n",
    "\n",
    "With the Soft Margin Classifier, you can tolerate a few points outside of the margin.\n",
    "\n",
    "<div>\n",
    "<img src=\"files/SVM_outliers.jpg\" width=\"55%\" source='https://roboticsbiz.com/pros-and-cons-of-support-vector-machine-svm/' align='center'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e4af8b-c464-4ca2-8d02-d8190b1d512f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hinge Loss\n",
    "\n",
    "The **Hinge Loss** is the penalty applied to each point on the wrong side.\n",
    "\n",
    "- The deeper a point lies within the margin, the higher the loss.\n",
    "- The penalty is linear, like MAE.\n",
    "\n",
    "<div>\n",
    "<img src=\"files/hinge_loss.png\" width=\"45%\" source='https://iq.opengenus.org/hinge-loss-for-svm/' align='center'/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c102d-e7f5-4280-b402-1391903b8b1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "When you're on the \"good side\" of the margin, the penalty is 0. Otherwise, when you cross the margin of your class, the penalty starts to increase.\n",
    "\n",
    "- How strong should the penalty be for wrongly classified datapoints?\n",
    "- How steep should the hinge loss be?\n",
    "- How narrow should the margin be?\n",
    "\n",
    "Once again, it's a tradeoff between classifying training data well and generalizing to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf53a363-e25f-4a58-b305-3b9ea6704a71",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Regularization with hyperparameter C\n",
    "\n",
    "Strength of the penalty applied on points located on the wrong side of the margin.\n",
    "\n",
    "- The higher $C$, the stricter the margin. A \"Maximum Margin Classifier\" has $C = +\\infty$.\n",
    "- The smaller $C$, the softer the margin, the more it is regularized. $C$ is similar to $\\frac{1}{\\alpha}$ in Ridge.\n",
    "\n",
    "<div>\n",
    "<img src=\"files/SVM_C.webp\" width=\"75%\" source='https://stackabuse.com/understanding-svm-hyperparameters/' align='center'/>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fffae60-5657-411a-bbf1-c7557e511889",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear', C=10)\n",
    "\n",
    "# equivalent but with SGD solver\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "svc_bis = SGDClassifier(loss='hinge', penalty='l2', alpha=1/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
